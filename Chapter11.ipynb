{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e11e64d-a87d-455c-ad50-9160df1961d6",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aded14e-1947-4623-81b6-bde8a5c809c8",
   "metadata": {},
   "source": [
    "Training a deep DNN isn’t a walk in the park. Here are \n",
    "some of the problems you could run into:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa1838-4de9-4bc4-bba7-6c827aae5e9c",
   "metadata": {},
   "source": [
    "You may be faced with the tricky vanishing gradients problem or the related exploding gradients problem. This is when the gradients grow smaller and smaller, or larger and larger, when flowing backward through the DNN during training. Both of these problems make lower layers very hard to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e553424-f0f8-4d26-83af-12b3761f8197",
   "metadata": {},
   "source": [
    "You might not have enough training data for such a large network, or it might be too costly to label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bce8d-e62d-4338-a168-3a612427943a",
   "metadata": {},
   "source": [
    "Training may be extremely slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932aa6e-d52f-4cc4-a4b3-86e1455f8f4d",
   "metadata": {},
   "source": [
    "A model with millions of parameters would severely risk overfitting the training \n",
    "set, especially if there are not enough training instances or if they are too noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "405b89cf-85ed-4f2e-a06e-40157cdf6423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c04fc-6f65-49dd-ae58-e4c7024fd99d",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b098e-9f8c-47ff-8472-8e83b26e9023",
   "metadata": {},
   "source": [
    "As we discussed in Chapter 10, the backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806edd9c-12ca-4b05-a350-c0b97512eaeb",
   "metadata": {},
   "source": [
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution. We call this the vanishing gradients problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients problem, which surfaces in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4504d8-9158-4f05-86b6-a47395bc4fcc",
   "metadata": {},
   "source": [
    "This unfortunate behavior was empirically observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000s. It wasn’t clear what caused the gradients to be so unstable when training a DNN, but some light was shed in a 2010 paper by Xavier Glorot and Yoshua Bengio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16414f6a-72a0-40a4-aef5-2aa6a20a5f52",
   "metadata": {},
   "source": [
    "### IMPORTANT POINT!!!!!!!!!!!!!!!!!!!!!!\n",
    "The authors found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c7972-a82b-4441-8e75-a60d752bdb92",
   "metadata": {},
   "source": [
    "#### Plotting the Logistic Sigmoid Activation Function\n",
    "Looking at the logistic activation function (see below), you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf48bf9-e326-408d-b08b-d98b0c4a083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f42c95e-eaee-4037-8488-988ee6369ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecfc277-adbf-4a94-8e43-8ff767867567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab483c6b-ddfc-4eb6-8a60-81f32f22a7a1",
   "metadata": {},
   "source": [
    "## Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94bfb5-b6b4-4818-ad5a-82153bc1689f",
   "metadata": {},
   "source": [
    "In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable gradients problem. They point out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097fc8eb-1f53-403c-9b43-1ebb9273d0a4",
   "metadata": {},
   "source": [
    "For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs(Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but if you set it too close to the max, your voice will be saturated and people won’t understand what you are saying. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude as it came in.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43986bb-bf05-4225-8c04-ed889d9f2e15",
   "metadata": {},
   "source": [
    "Also, we need the gradients to have equal variance before and after flowing through a layer in the reverse direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885bf55-34b8-4927-9a37-510e4dba68ba",
   "metadata": {},
   "source": [
    "It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the fan-in and fan-out of the layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15ccf3-1af4-482f-88b9-1b8a3283d0d5",
   "metadata": {},
   "source": [
    "#### Initialization Equation\n",
    "\n",
    "But Glorot and Bengio proposed a good compro mise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as described in equation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b812a-d827-4d1d-9edc-2f4ca2b5b368",
   "metadata": {},
   "source": [
    "#### Equation :- Glorot initialization (when using the logistic activation function)\n",
    "\n",
    "fanavg = (fanin + fanout)/2\n",
    "\n",
    "Normal distribution with mean 0 and variance σ2 = 1 / fan avg \r\n",
    "\r\n",
    "Or a uniform distribution between −r and + r, with r = sqrt (3 /fan avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95310956-da94-4057-ba13-8536ca45a972",
   "metadata": {},
   "source": [
    "If you replace fanavg with fanin in the above Equation, you get an initialization strategy that Yann LeCun proposed in the 1990s. He called it LeCun initialization. Genevieve Orr and Klaus-Robert Müller even recommended it in their 1998 book Neural Networks: Tricks of the Trade (Springer). LeCun initialization is equivalent to Glorot initialization when fanin = fanout . It took over a decade for researchers to realize how important this trick is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91962f29-ca38-44d8-8953-151a45e4d97e",
   "metadata": {},
   "source": [
    "#### See table 11.1 in the textbook for different initialization strategy and the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8188d4-6c19-48ce-a6ea-908b43b74bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebfdeb06-586b-4422-b950-a70b95723fd1",
   "metadata": {},
   "source": [
    "By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, you can change this to He initialization by setting kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\" like this:\n",
    "\n",
    " keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d567a93-9a7f-48c5-b05e-04ab1fc8d768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f565aae-7f2e-4011-901a-36cf3fd41f34",
   "metadata": {},
   "source": [
    "If you want He initialization with a uniform distribution but based on fanavg rather than fanin , you can use the VarianceScaling initializer like this:\n",
    "\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',  distribution='uniform')\n",
    "\n",
    " keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f97f46e-9b99-4227-958f-3f4da6c87544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05e076a-cf91-490a-be45-8eecc3ee46de",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3071485-6413-45f2-925e-618cdafe7507",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c035087-6558-4b84-b3a3-7e7c2b8271ea",
   "metadata": {},
   "source": [
    "### ReLU activation function behaves much better in deep neural networks in particular, mostly because it does not saturate for positive values (and because it is fast to compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa09912-bcc0-423f-8718-2f64dd775dad",
   "metadata": {},
   "source": [
    "### PROBLEM OF RELU!!!!!\n",
    "\n",
    "Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303d322-113e-4c97-a8ee-fdb250d9be75",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: Gradient Descent may indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s inputs is positive again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbc5c3-433b-4ff0-b8f9-014d49ad02f1",
   "metadata": {},
   "source": [
    "#### Plotting ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360b5dd9-4eb9-4ffc-af0b-0d9c7b890794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405c78d9-060f-45fd-bdb3-e47d398fe62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\" ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86220b0d-81ba-422f-ad07-8803bbf7eeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6385836c-475e-44ab-8c46-0c042077858c",
   "metadata": {},
   "source": [
    "### LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87ce7d-e1d4-41ef-9ad5-4124ad17997d",
   "metadata": {},
   "source": [
    "To solve this problem, you may want to use a variant of the ReLU function, such as the leaky ReLU. This function is defined as LeakyReLUα(z) = max(αz, z). The hyperparameter α defines how much the function “leaks”: it is the slope of the function for z < 0 and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db7773-1d47-47b2-ac5c-ebc4c68b8f45",
   "metadata": {},
   "source": [
    "#### Plotting Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e2019c-b919-4e56-bf45-9d8d53b38a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67de75cc-a05a-4860-b92a-cfae54526dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e4ecd-0914-4a51-91ff-c1982fa027af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5aa0304-c034-4fa8-8e8c-5aab9dda23fe",
   "metadata": {},
   "source": [
    "### Versions of ReLU\n",
    "\n",
    "The paper Bing Xu et al., “Empirical Evaluation of Rectified Activations in Convolutional Network” compared several variants of the ReLU activation function, and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting α = 0.2 (a huge leak) seemed to result in better performance than α = 0.01 (a small leak). The paper also evaluated the randomized leaky ReLU (RReLU), where α is picked randomly in a given range during training and is fixed to an average value during testing. RReLU also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set). Finally, the paper evaluated the parametric leaky ReLU (PReLU), where α is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b69f0d-cb7c-4a2e-b005-8c6129340a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e5ff4f9-c540-4755-ac7e-4454bd05e29c",
   "metadata": {},
   "source": [
    "## ELU (Exponential Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed2a11-f4c0-43a1-938d-127f93bf383c",
   "metadata": {},
   "source": [
    "The 2015 paper by Djork-Arné Clevert et al proposed a new activation function called the exponential linear unit (ELU) that outperformed all the ReLU variants in the authors’ experiments: training time was reduced, and the neural network performed better on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36428c0-174c-47aa-95f4-4d1224c22bb7",
   "metadata": {},
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab1e9a-c682-4a73-baec-5e56823bc400",
   "metadata": {},
   "source": [
    "### ELU equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ddc35-f09a-459a-b762-1f413562108a",
   "metadata": {},
   "source": [
    "ELUαz = α (exp (z) − 1) if z < 0 or z if z ≥ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f49d02-27d2-41b6-bee4-2b4a98c4c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d81f8007-27c5-4467-ad5a-b7a70aa6e6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d116610-4ca7-4599-87fb-50b68463d86e",
   "metadata": {},
   "source": [
    "### Charactersitics of ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2412192-f81d-47ef-a550-6fcdf7db904b",
   "metadata": {},
   "source": [
    "The ELU activation function looks a lot like the ReLU function, with a few major differences:\n",
    "1. It takes on negative values when z < 0, which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter α defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter.\n",
    "2. It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n",
    "3. If α is equal to 1 then the function is smooth everywhere, including around z = 0, which helps speed up Gradient Descent since it does not bounce as much to the left and right of z = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691f266-95ce-4687-9c26-b2759fcfe719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efc62b96-66b2-467f-9cb2-1e1faae1a094",
   "metadata": {},
   "source": [
    "### Drawbacks of ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc458623-e641-4ef4-938f-31f9827187f0",
   "metadata": {},
   "source": [
    "The main drawback of the ELU activation function is that it is slower to compute than the ReLU function and its variants (due to the use of the exponential function). Its faster convergence rate during training compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7ced0-31c1-4eb0-88f0-fad84d9071ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d282069-cdb1-42b1-9140-5afea3552a08",
   "metadata": {},
   "source": [
    "## SELU Scaled ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22818a00-007f-4940-abb2-07ee7e0b2813",
   "metadata": {},
   "source": [
    "As the name suggests, it is a scaled variant of the ELU activation function. The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will self-normalize: the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e681716-1d10-45ad-8d97-6d12b2da3064",
   "metadata": {},
   "source": [
    "#### Pre-conditions for SELU\n",
    "\n",
    "For the SELU to work as awesome, There are, however, a few conditions for self-normalization to happen :\n",
    "1. The input features must be standardized (mean 0 and standard deviation 1).\n",
    "2. Every hidden layer’s weights must be initialized with LeCun normal initialization. In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
    "3. The network’s architecture must be sequential. Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks or networks with skip connections (i.e., connections that skip layers, such as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633531c-1a42-4ec0-87db-f9aeb8756de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f18cf6c-c3fa-4371-9f8a-f5c442499593",
   "metadata": {},
   "source": [
    "# WHICH ACTIVATION FUNCTION TO USE???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6a0a3-6b04-417f-b8ff-e3f47cbd88d1",
   "metadata": {},
   "source": [
    "So, which activation function should you use for the hidden layers of your deep neural networks? Although your mileage will vary, in general follow the below\r\n",
    "\r\n",
    "SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020433b-be7a-4c9d-9d4e-050e3daaa1a1",
   "metadata": {},
   "source": [
    "If the network’s architecture prevents it from selfnormalizing, then ELU may perform better than SELU (since SELUis not smooth at z = 0). If you care a lot about runtime latency, then you may prefer leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f885d27-5033-4994-b428-a569c459f4c6",
   "metadata": {},
   "source": [
    "If you don’t want to tweak yet another hyperparameter, you may use the default α values used by Keras (e.g., 0.3 for leaky ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582ed8b-cbc4-436a-b77a-9a9b9e2a9ec8",
   "metadata": {},
   "source": [
    "If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, such as RReLU if your network is overfitting or PReLU if you have a huge training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f03d0-4604-4a9e-9290-768285dddd50",
   "metadata": {},
   "source": [
    "That said, because ReLU is the most used activation function (by far), many libraries and hardware accelerators provide ReLU-specific optimizations; therefore, if speed is your priority, ReLU might still be the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d3c78-0866-4c77-9645-dd0e3d8f637a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16ca0512-4738-4ac5-a371-f09be04148af",
   "metadata": {},
   "source": [
    "### Using the Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfdbd06-57ad-4c84-a70f-a5bb4eb57bed",
   "metadata": {},
   "source": [
    "#### Leaky ReLU\n",
    "\n",
    "To use the leaky ReLU activation function, create a LeakyReLU layer and add it to your model just after the layer you want to apply it to:\n",
    "model = keras.models.Sequential([\r\n",
    " [...]\r\n",
    " keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\r\n",
    " keras.layers.LeakyReLU(alpha=0.2),\r\n",
    " [...]\r\n",
    "])\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2e8a6-f7b9-48c5-b8b0-b787e599d0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e461f48c-7740-4036-9b16-dc6de2c31a54",
   "metadata": {},
   "source": [
    "#### SELU\n",
    "For SELU activation, set activation=\"selu\" and kernel_initializer=\"lecun_normal\" when creating a layer:\r\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e8a06-5dd0-4b1c-93aa-8c2e7174f98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4c95672-5232-41b5-99f9-a7293addf445",
   "metadata": {},
   "source": [
    "# BATCH NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c6a75-ab93-4e6b-9af3-867820426813",
   "metadata": {},
   "source": [
    "In a 2015 paper,Sergey Ioffe and Christian Szegedy proposed a technique called Batch Normalization (BN) that addresses these problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d8a0a-af3f-4c6f-9836-43b33f2ed12c",
   "metadata": {},
   "source": [
    "This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer’s inputs. The part \"In other words, the operation lets the model learn the optimal scale and mean of each of the layer’s inputs\" means that the model is also learning what is the scale and mean of the inputs that are being passed to the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762beb28-3054-411e-9100-4b0afcd80530",
   "metadata": {},
   "source": [
    "In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72324338-8673-415c-84b1-415dc28a8e0b",
   "metadata": {},
   "source": [
    "#### How to batch normalize, i.e. zero center and normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738827c-c3a8-4286-963e-2bc87b7a3eed",
   "metadata": {},
   "source": [
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd0ac8-dca2-4394-a0f0-fc0d978f520d",
   "metadata": {},
   "source": [
    "### IMPORTANT!!!! Refer the Batch Normalization Algorithm from the textbook, pg 339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055acfa6-ca7d-42cf-9c56-a8dacfae9527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03ee3fd-4c5a-4af0-bd9c-a1af3b6795ff",
   "metadata": {},
   "source": [
    "### Batch Normalization during Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3dd26-88dc-4e6e-9842-ac29c42a7fd1",
   "metadata": {},
   "source": [
    "#### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146864b-f9b7-484a-af1b-034d56ff35f5",
   "metadata": {},
   "source": [
    "So during training, BN standardizes its inputs, then rescales and offsets them. Good! What about at test time? Well, it’s not that simple. Indeed, we may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160f767-3727-4ddf-b0f5-8db39a0e95f6",
   "metadata": {},
   "source": [
    "Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab7c8d-3f32-4b46-9339-13e1cc630fab",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2f006-91e6-4e8e-81f9-c626019e64a1",
   "metadata": {},
   "source": [
    "One solution could be to wait until the end of training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations could then be used instead of the batch input means and standard deviations when making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52645d7c-e77f-407c-b75f-ee0ad0440084",
   "metadata": {},
   "source": [
    "#### Solution Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b72c9-b803-4063-9d00-2bcfb99b7fdd",
   "metadata": {},
   "source": [
    "However, most implementations of Batch Normalization estimate these final statistics during training by using a moving average of the layer’s input means and standard deviations. This is what Keras does automatically when you use the BatchNormalization layer. To sum up, four parameter vectors are learned in each batch-normalized layer: γ (the output scale vector) and β (the output offset vector) are learned through regular backpropagation, and μ (the final input mean vector) and σ (the final input standard deviation vector) are estimated using an exponential moving average. Note that μ and σ are estimated during training, but they are used only after training (to replace the batch input means and standard deviations in the batch normalization algorithm equation on pg 339)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324af54-09ee-4ba8-b56c-52de3713775a",
   "metadata": {},
   "source": [
    "#### Batch Normalization as Regularization\n",
    "Batch Normalization acts like a regularizer, reducing the need for other regularization techniques (such as dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b3bf0-c78c-48a1-996e-de09c7e0953d",
   "metadata": {},
   "source": [
    "## Complexity due to Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aad8c7-f2eb-47d1-a670-4ec367b1e24b",
   "metadata": {},
   "source": [
    "Batch Normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data, as we discussed earlier). Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f2284-8d1b-4306-9db7-8d5ff4911300",
   "metadata": {},
   "source": [
    "### Solving the complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c6a80-c624-4081-88b8-be47858bafb1",
   "metadata": {},
   "source": [
    "Fortunately, it’s often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty. This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset.et."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c9ee4-709b-4126-9482-95628af5e3ca",
   "metadata": {},
   "source": [
    "For example, if the previous layer computes XW + b, then the BN layer will compute γ⊗(XW + b – μ)/σ + β (ignoring the smoothing term ε in the denominator). If we define W′ = γ⊗W/σ and b′ = γ⊗(b – μ)/σ + β, the equation simplifies to XW′ + b′. So if we replace the previous layer’s weights and biases (W and b) with the updated weights and biases (W′ and b′), we can get rid of the BN layer (TFLite’s optimizer does this automatically)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dffcd0-1d79-404d-b309-7f49c14435d2",
   "metadata": {},
   "source": [
    "#### General Note Batch Normalization\n",
    "You may find that training is rather slow, because each epoch takes much more time when you use Batch Normalization. This is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach the same performance. All in all, wall time will usually be shorter (this is the time measured by the clock on your wall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066b471-ec41-4c89-ab01-30bda24cf94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eb39a80-a746-4f7b-8c79-3f0130dc3c6d",
   "metadata": {},
   "source": [
    "### Implementing Batch Normalization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18743cf3-6d21-474d-b146-8f8849a0521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python 3.11\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape=[28, 28]),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a19db4-a3da-41ac-b25a-dc6986e3ede7",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3084aa02-79d6-4d24-af86-c7c96b8e7fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 784)               3136      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae9e0e-8182-47e2-bb46-e4219bcd9908",
   "metadata": {},
   "source": [
    "#### Model Explanation IMPORTANT!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e3803-3283-4195-9963-b391b5abc496",
   "metadata": {},
   "source": [
    "As you can see, each BN layer adds four parameters per input: γ, β, μ, and σ (for example, the first BN layer adds 3,136 parameters, which is 4 × 784). The last two parameters, μ and σ, are the moving averages; they are not affected by backpropagation, so Keras calls them “non-trainable” (if you count the total number of BN parameters, 3,136 + 1,200 + 400, and divide by 2, you get 2,368, which is the total number of non-trainable parameters in this model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de9470-8bd8-4166-8131-92fdab80fedc",
   "metadata": {},
   "source": [
    "### Note about Non-trainable\n",
    "\n",
    "In the above example, although, μ and σ are marked as non-trainable by keras, However, they are estimated during training, based on the training data, so arguably they are trainable. In Keras, “non-trainable” really means “untouched by backpropagation.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc19d7a-6a84-49e7-91cb-d6fbe900345c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "167381d8-2cda-4533-bfcc-ae824632190b",
   "metadata": {},
   "source": [
    "#### Analysing BN layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854b33a-8a1c-4230-977a-9a45a5b6e1a2",
   "metadata": {},
   "source": [
    "Let's look at the first BN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ca0fdd-3d38-4626-8930-47eca42bb23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f47c1-2ae6-4c1e-b37d-93cd1bd0d684",
   "metadata": {},
   "source": [
    "True means trainable by backpropogation, False means non-trainable. Hence, two are trainable and two are not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe647922-cc43-4728-855d-d7bbb8ac54ba",
   "metadata": {},
   "source": [
    "#### Note about TF Backend\n",
    "\n",
    "Now when you create a BN layer in Keras, it also creates two operations that will be called by Keras at each iteration during training. These operations will update the moving averages. Since we are using the TensorFlow backend, these operations are TensorFlow operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95a67e-489d-4c02-875c-1d71942498d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b99c8b0-4e13-43f3-8c0c-1386f0cbe7e4",
   "metadata": {},
   "source": [
    "## Where to add Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453489e-c98c-4de8-92b5-181a4a26dca3",
   "metadata": {},
   "source": [
    "The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after (as we just did)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477348c7-18a3-42a7-9fd6-759e42fe14cf",
   "metadata": {},
   "source": [
    "There is some debate about this, as which is preferable seems to depend on the task—you can experiment with this too to see which option works best on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665727b-4889-4cb4-a538-db322efacc69",
   "metadata": {},
   "source": [
    "#### Adding Batch Normalization before Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b03486-ef69-4762-bdcf-6c3924a66dc8",
   "metadata": {},
   "source": [
    "To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since a Batch Normalization layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass use_bias=False when creating it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7bf915c-945b-421d-8d3f-a18bad2225ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape=[28, 28]),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Activation(\"elu\"),\n",
    " keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Activation(\"elu\"),\n",
    " keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85396a01-74a0-4fc1-9646-3a877da8d3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d02b4fa-10dd-4c99-a7f5-b284e0834c5d",
   "metadata": {},
   "source": [
    "## Batch Normalization Parent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397a0c0-a249-448f-9d5c-d30c4c343e04",
   "metadata": {},
   "source": [
    "The BatchNormalization class has quite a few hyperparameters you can tweak. The defaults will usually be fine, but you may occasionally need to tweak the momentum. This hyperparameter is used by the BatchNormalization layer when it updates the exponential moving averages; given a new value v (i.e., a new vector of input means or standard deviations computed over the current batch), the layer updates the running average v^ using the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f70c40-248e-4a73-85d7-e32a42c235c9",
   "metadata": {},
   "source": [
    "#### Equation for updating the running average using the momentum\n",
    "v^ <- v^ × momentum + v × (1 − momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2777c-ac43-48bd-8017-eeee71691e5c",
   "metadata": {},
   "source": [
    "##### Good momentum value\n",
    "A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.999 (you want more 9s for larger datasets and smaller mini-batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501e885-9c4e-407b-a78f-c9af1fac4713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab62b436-f3eb-45fc-87dc-aca7fc7fa413",
   "metadata": {},
   "source": [
    "### Batch Normalization Hyperparameter Axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd1d27-8099-44ee-8a7f-af814c280d55",
   "metadata": {},
   "source": [
    "Another important hyperparameter is axis: it determines which axis should be normalized. It defaults to –1, meaning that by default it will normalize the last axis (using the means and standard deviations computed across the other axes). When the input batch is 2D (i.e., the batch shape is [batch size, features]), this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch.\n",
    "\n",
    "### Important\n",
    "Now note that batch size by default is 32, or can be defined separately in fit(), here the part \"input batch is 2D (i.e., the batch shape is [batch size, features])\" means the None, 784 part that is present in the first layer for model.summary in Implementing Batch Normalization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232aee7-10ab-4560-adec-7d912133b3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7311a65e-1be2-4c8b-9d30-dfa1cb872378",
   "metadata": {},
   "source": [
    "### 2D Batch Size for Batch Normalization\n",
    "When the input batch is 2D (i.e., the batch shape is [batch size, features]), like the None, 784 just mentioned before, this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch. For example, the first BN layer in the previous code example will independently normalize (and rescale and shift) each of the 784 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91892d1-3f23-4500-ba57-096ac9b94b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "481c938a-d908-40f2-97b2-71d048f9ab03",
   "metadata": {},
   "source": [
    "### 3D Batch Size for Batch Normalization\n",
    "If we move the first BN layer before the Flatten layer, then the input batches will be 3D, with shape [batch size, height, width]; therefore, the BN layer will compute 28 means and 28 standard deviations (1 per column of pixels, computed across all instances in the batch and across all rows in the column), and it will normalize all pixels in a given column using the same mean and standard deviation. There will also be just 28 scale parameters and 28 shift parameters. If instead you still want to treat each of the 784 pixels independently, then you should set axis=[1, 2].\n",
    "\n",
    "### Explanation for 3D\n",
    "Explaining this part, now you are not flattening the input before passing it to Batch Normalization, hence what BN gets is [32,28,28] which basically is the default batch size with 28x28 2D image pixels for each image. Hence since by default BN uses -1 i.e. the last axis, which basically means the last value that is there in the [32,28,28], it will just calculate 28 means and 28 standard deviations (1 per column of pixels, computed across all instances in the batch and across all rows in the column), and it will normalize all pixels in a given column using the same mean and standard deviation. This is acually reducing the mean and the standard deviation from the before 784 count. \n",
    "\n",
    "#### Explanation for axis = [1,2]\n",
    "Now, in the above example, how does axis = [1,2] instructs BN to reat each of the 784 pixels independently? so when you pass [1,2], it tells BN that from the 3D input batches that you get which are [32,28,28], select the values that are at 1st and 2nd index, which 28,28 in our case. Then as you already know that All axes not listed in axis are the ones over which BN will compute the mean/variance. The axes in axis are the ones that define which features get distinct normalization. Hence, BatchNormalization(axis=[1, 2]), BN computes:mean = input.mean(axis=0) \n",
    "std  = input.std(axis=0 which means 32 in our case, hence it will take 28,28 together, and compute mean and standard deviation over each batch of 32 inputs, like it did when we put a flatten layer before the BN layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6121b-7ba2-4624-8af3-83f0acd69b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f843e2bc-6fd4-4586-8359-b4e8e7660382",
   "metadata": {},
   "source": [
    "### Batch Normalization Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e916600-0bfd-4d3b-b42e-3ca69cdc89f8",
   "metadata": {},
   "source": [
    "BatchNormalization has become one of the most-used layers in deep neural networks, to the point that it is often omitted in the diagrams, as it is assumed that BN is added after every layer. But a recent paper11 by Hongyi Zhang et al. may change this assumption: by using a novel fixed-update (fixup) weight initialization technique, the authors managed to train a very deep neural network (10,000 layers!) without BN, achieving state-of-the-art performance on complex image classification tasks. As this is bleeding-edge research, however, you may want to wait for additional research to confirm this finding before you drop Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb351ef-949b-4996-8550-d28fb1eec225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc2e99-a810-4624-9cf2-8238f0339102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "657569c6-c4c0-481c-95b9-e0dc686d681a",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb820bc9-6fb2-4a4a-8568-1c11420fb5af",
   "metadata": {},
   "source": [
    "Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This is called Gradient Clipping. This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs. For other types of networks, BN is usually sufficient.\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791350e-cfd5-49e1-860e-da3a695f896f",
   "metadata": {},
   "source": [
    "In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or clipnorm argument when creating an optimizer, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c84dc5-5782-4edb-acba-1e7aa8f03888",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bddb843-a694-4664-8383-5cd6b0fb707a",
   "metadata": {},
   "source": [
    "This optimizer will clip every component of the gradient vector to a value between –1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyperparameter you can tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899172f-e8f2-4502-805d-960df3e04962",
   "metadata": {},
   "source": [
    "### Changing the direction of Gradient due to Gradient Clipping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b245f219-649d-4017-bdd3-2fda2877c9f4",
   "metadata": {},
   "source": [
    "Note that it may change the orientation of the gradient vector. For instance, if the original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. In practice, this approach works well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad359211-bca9-4b19-8ea3-70759d81f5b2",
   "metadata": {},
   "source": [
    "### Avoiding changing the direction of the gradient vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fead7b1-6953-4874-ab66-41d50a06b169",
   "metadata": {},
   "source": [
    "If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue. This will clip the whole gradient if its ℓ2 norm is greater than the threshold you picked.\n",
    "\n",
    "For example, if you set clipnorm=1.0, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f228395-cb54-478a-b0d8-4be96dfa256e",
   "metadata": {},
   "source": [
    "#### Note gradient clipping\n",
    "If you observe that the gradients explode during training (you can track the size of the gradients using TensorBoard), you may want to try both clipping by value and clipping by norm, with different thresholds, and see which option performs best on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64137467-5b1f-4408-adc1-587bdfcf78fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e36794-c564-445f-9071-5405199d5f40",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11d5b3-8d16-40aa-8fc4-a4115657a2f4",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle then reuse the lower layers of this network. This technique is called transfer learning. It will not only speed up training considerably, but also require significantly less training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa04e9-6769-4222-96ad-6a5be36f3cc1",
   "metadata": {},
   "source": [
    "#### Example scenario for using pretrained layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf5870-f26e-4bf4-930d-3d5edc77c97d",
   "metadata": {},
   "source": [
    "Suppose you have access to a DNN that was trained to classify pictures into 100 different categories, including animals, plants, vehicles, and everyday objects. You now want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so you should try to reuse parts of the first network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec737209-25b1-4408-ae53-c0b3a394acf4",
   "metadata": {},
   "source": [
    "The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b818e6d-ad45-4174-91c4-72dd7b1f1a13",
   "metadata": {},
   "source": [
    "Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. You want to find the right number of layers to reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd951a-cd05-497b-93af-18e48ad58e7d",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6254b3-303c-423c-b61e-7064d1e47d4a",
   "metadata": {},
   "source": [
    "### Important warning about Transfer Learning\n",
    "If the input pictures of your new task don’t have the same size as the ones used in the original task, you will usually have to add a preprocessing step to resize them to the size expected by the original model. More generally, transfer learning will work best when the inputs have similar low-level features.\r\n",
    "es.\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a397b-a1de-4f41-b7ca-aaaef4016b56",
   "metadata": {},
   "source": [
    "### Pro-Tip Transfer Learning\n",
    "The more similar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replacing the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1a70c-0233-4b5b-b85b-bd88ceb81420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "774f4e01-fca5-453d-9c3e-4965d8a7a866",
   "metadata": {},
   "source": [
    "### Transfer Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a2cdd-952e-4cee-8f82-0633127710b9",
   "metadata": {},
   "source": [
    "Try freezing all the reused layers first (i.e., make their weights non-trainable so that Gradient Descent won’t modify them), then train your model and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e1d00-c985-44e6-a88d-f0ac2f94d24b",
   "metadata": {},
   "source": [
    "Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2ecc6-1936-4937-a9f6-e0c528285cc9",
   "metadata": {},
   "source": [
    "The more training data you have, the more layers you can unfreeze. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109f9a4-5d76-4dc2-b1e5-809cd14119d3",
   "metadata": {},
   "source": [
    "#### Important Note about the Learning rate when using pretrained layers and unfreezing them!!!\n",
    "\n",
    "It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd234d12-1ebd-46c5-89af-f1e9a2d3e2d4",
   "metadata": {},
   "source": [
    "### Transfer Learning how to improve performance\n",
    "If you still cannot get good performance, and you have little training data, try drop‐\r\n",
    "ping the top hidden layer(s) and freezing all the remaining hidden layers again. You\r\n",
    "can iterate until you find the right number of layers to reuse. If you have plenty of\r\n",
    "training data, you may try replacing the top hidden layers instead of dropping them,\r\n",
    "and even adding more hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f9658-58d8-48a7-9ae9-cf68a70ba69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "676db9fa-11c5-4273-bad3-ef2ebec5a549",
   "metadata": {},
   "source": [
    "## Transfer Learning with KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7397cc6-084a-4335-9d10-ae9895175b56",
   "metadata": {},
   "source": [
    "Let’s look at an example. Suppose the Fashion MNIST dataset only contained eight classes—for example, all the classes except for sandal and shirt. Someone built and trained a Keras model on that set and got reasonably good performance (>90% accuracy). Let’s call this model A. You now want to tackle a different task: you have images of sandals and shirts, and you want to train a binary classifier (positive=shirt, negative=sandal). Your dataset is quite small; you only have 200 labeled images. When you train a new model for this task (let’s call it model B) with the same architecture as model A, it performs reasonably well (97.2% accuracy). But since it’s a much easier task (there are just two classes), you were hoping for more. While drinking your morning coffee, you realize that your task is quite similar to task A, so perhaps transfer learning can help? Let’s find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751a0ee-f903-4b04-8e04-f911480d6e5c",
   "metadata": {},
   "source": [
    "#### Loading model A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a53234-9c9e-40ab-bb8e-ebad284aa676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python 3.11\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_A = keras.models.load_model(\"my_keras_best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc01745a-57fd-415f-8340-e50e818c980f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.core.dense.Dense at 0x207da1b9c90>,\n",
       " <keras.src.layers.core.dense.Dense at 0x20785a49890>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c468183-8c87-4703-bf72-45426a2469f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 30)                270       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301 (1.18 KB)\n",
      "Trainable params: 301 (1.18 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca433a3c-2688-408a-9fb6-d83038153c25",
   "metadata": {},
   "source": [
    "#### Creating model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88d0d007-fc10-479d-8646-029ed4ef4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbd3b9-5ba7-485a-8ef8-5f9851452657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78ac1c6b-e5da-4e19-a28b-6478ae5d4c22",
   "metadata": {},
   "source": [
    "#### WARNING ABOUT TRAINING IN TRANSFER LEARNING!!\n",
    "model_A and model_B_on_A now share some layers. When you train model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone model_A before you reuse its layers. To do this, you clone model A’s architecture with clone_model(), then copy its weights (since clone_model() does not clone the weights):\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad420b44-c270-4202-982b-4c17e958d2b0",
   "metadata": {},
   "source": [
    "#### Cloning model A to preserve it's weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c52da19-ab04-41a3-8f78-ec06a974e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d456bc-2154-487c-9b4d-2c411ea32b36",
   "metadata": {},
   "source": [
    "### Training model B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba8f65-289b-42a0-9b89-aae009dcdd95",
   "metadata": {},
   "source": [
    "Now you could train model_B_on_A for task B, but since the new output layer was initialized randomly it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, set every layer’s trainable attribute to False and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15f5f442-8929-4818-a665-8aec642d91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72578de5-fb1b-4e45-8c3f-55263eda580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\n",
    " metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad44ad-db29-4542-a656-a84e8fd571b4",
   "metadata": {},
   "source": [
    "#### Compiling model WARNING!!!!!!!!!!!!!! You must always compile your model after you freeze or unfreeze layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736218aa-ff91-47fb-97f8-9a4bc6fcc30d",
   "metadata": {},
   "source": [
    "Now you can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f02204-f18d-4028-a876-55c5a69f98e1",
   "metadata": {},
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\r\n",
    " validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "360ab399-9635-4e4e-b102-b1830c399cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c3fee-a605-4007-aa57-ff13a692257c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d68b222c-ba14-4171-9bf5-92d1d2ae27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4) # the default lr is 1e-2\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    " metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fa5b7-700c-45d0-83f7-ba8903d793ef",
   "metadata": {},
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\r\n",
    " validation_data=(X_valid_B, y_valid_B))\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb87cc-1439-4f69-9406-6188b4ff5f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e15d2fbe-ea57-4f9c-bf52-2858eec8abe1",
   "metadata": {},
   "source": [
    "## Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54285c78-5add-4f24-9cab-f8098b135a83",
   "metadata": {},
   "source": [
    "#### Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb346c-c531-49ed-ac3b-6c3948fd0094",
   "metadata": {},
   "source": [
    "Suppose you want to tackle a complex task for which you don’t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don’t lose hope! First, you should try to gather more labeled training data, but if you can’t, you may still be able to perform unsupervised pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd66dad-1afa-4f88-a0d3-b392477be8e6",
   "metadata": {},
   "source": [
    "#### Solution to the Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e159750-0bd0-4848-8341-c2932fd689a6",
   "metadata": {},
   "source": [
    "Indeed, it is often cheap to gather unlabeled training examples, but expensive to label them. If you can gather plenty of unlabeled training data, you can try to use it to train an unsupervised model, such as an autoencoder or a generative adversarial network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d141c-d64b-4d29-a35f-03857fcc3f4e",
   "metadata": {},
   "source": [
    "Then you can reuse the lower layers of the autoencoder or the lower layers of the GAN’s discriminator, add the output layer for your task on top, and finetune the final network using supervised learning (i.e., with the labeled training examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f297b-7da4-4efd-bf8a-978795b14e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff4bfe72-bc41-40fc-a6fb-5afb18c69c7d",
   "metadata": {},
   "source": [
    "## Pretraining on an Auxiliary Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcef38-ad37-4206-97f8-b69c7f48c1cd",
   "metadata": {},
   "source": [
    "If you do not have much labeled training data, one last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The first neural network’s lower layers will learn feature detectors that will likely be reusable by the second neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4a21b-bc13-45ce-a2c6-b812af702c02",
   "metadata": {},
   "source": [
    "For example, if you want to build a system to recognize faces, you may only have a few pictures of each individual—clearly not enough to train a good classifier. Gathering hundreds of pictures of each person would not be practical. You could, however, gather a lot of pictures of random people on the web and train a first neural network to detect whether or not two different pictures feature the same person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f4890-b48e-4f70-a0a6-58ddb15da538",
   "metadata": {},
   "source": [
    "Such a network would learn good feature detectors for faces, so reusing its lower layers would allow you to train a good face classifier that uses little training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1f6f0-a692-4e7d-9beb-c824ac644d87",
   "metadata": {},
   "source": [
    "#### NLP in Auxiliary learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae15a7e-70fa-4f82-b764-aa4962523d4d",
   "metadata": {},
   "source": [
    "For natural language processing (NLP) applications, you can download a corpus of millions of text documents and automatically generate labeled data from it. For example, you could randomly mask out some words and train a model to predict what the missing words are (e.g., it should predict that the missing word in the sentence “What ___ you saying?” is probably “are” or “were”). If you can train a model to reach good performance on this task, then it will already know quite a lot about language, and you can certainly reuse it for your actual task and fine-tune it on your labeled data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e9e30-576c-4a0e-b75a-0cfe4edfebf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2774d39-cec4-4450-beb7-0b9bb1917315",
   "metadata": {},
   "source": [
    "### Self-Supervised Learning\n",
    "Self-supervised learning is when you automatically generate the labels from the data itself, then you train a model on the resulting “labeled” dataset using supervised learning techniques. Since this approach requires no human labeling whatsoever, it is best classified as a form of unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695cef1-cef4-4db4-a3f9-895b5f71178b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3d40b30-3b1f-4c10-b405-038524488fe7",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125bf412-3fd6-452a-89a6-3f45171cb496",
   "metadata": {},
   "source": [
    "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution): applying a good initialization strategy for the connection weights, using a good activation function, using Batch Normalization, and reusing parts of a pretrained network (possibly built on an auxiliary task or using unsupervised learning). Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer. In this section we will present the most popular algorithms: momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a341c1e-13ad-464f-ab44-12471e8425ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d3c41be-c4e4-48f4-a83e-ca3ed12a49ce",
   "metadata": {},
   "source": [
    "## Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809f3c3-005b-48e6-b556-f23527bf8b27",
   "metadata": {},
   "source": [
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind momentum optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487bea47-44c9-4b58-8a5d-60fda49b139b",
   "metadata": {},
   "source": [
    "##### How is regular gradient descent different from Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987dbfc6-2309-471e-8965-b1a076fbf605",
   "metadata": {},
   "source": [
    "a regular Gradient Descent simply takes small, regular steps down the slope, so the algorithm will take much more time to reach the bottom.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd4c5f-5d60-46cc-bad2-d41ae7576c65",
   "metadata": {},
   "source": [
    "##### How Gradient Descent updates the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1102e579-38c0-4627-bd2f-ca3f2fd8639e",
   "metadata": {},
   "source": [
    "Recall that Gradient Descent updates the weights θ by directly subtracting the current weights with gradient of the cost function J(θ) with regard to the current weights i.e. (∇θ J(θ)) multiplied by the learning rate η. The equation is: \r\n",
    "\r\n",
    "θ ← θ – η∇θJ(θ) \r\n",
    "\r\n",
    "It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a7163-1165-4853-bf05-9052b0fdeeac",
   "metadata": {},
   "source": [
    "#### How Momentum Optimization works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d589f-0e42-41d3-9d3b-623c09f919cf",
   "metadata": {},
   "source": [
    "Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m, and it updates the weights by adding this momentum vector (see Equation below).\n",
    "\n",
    "Step1 :- m <- βm − η∇θJ(θ)\r\n",
    "\n",
    "Step2 :- \n",
    "θ <- θ + \n",
    "\n",
    "To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter β, called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d917f-4187-4513-9ac4-62bb478020f8",
   "metadata": {},
   "source": [
    "#### Momentum Optimization working example(couldn't understand this)\n",
    "You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate η multiplied by 1/(1–β) (ignoring the sign). For example, if β = 0.9, then the terminal velocity is equal to 10 times the gradient times the learning rate, so momentum optimization ends up going 10 times faster than Gradient Descent! This allows momentum optimization to escape from plateaus much faster than Gradient Descent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950157cb-b74a-41b4-bf8d-5ba38208083f",
   "metadata": {},
   "source": [
    "#### Momentum Optimization working Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08d090e5-3894-427e-aa8f-f5d40c71425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d82f9c-3f8d-436f-8f1d-59d7d96e830c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca2a8ac-e501-46a0-bec4-6ecc5485ffa7",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e9638-af09-4709-887e-7023c7e5c5de",
   "metadata": {},
   "source": [
    "One small variant to momentum optimization, proposed by Yurii Nesterov in 1983, is almost always faster than vanilla momentum optimization. The Nesterov Accelerated Gradient (NAG) method, also known as Nesterov momentum optimization, measures the gradient of the cost function not at the local position θ but slightly ahead in the direction of the momentum, at θ + βm (see Equation below).\n",
    "\n",
    "Equation Nesterov Accelerated Gradient algorithm\n",
    "\n",
    "\n",
    "Step1 : m <- βm − η∇θ J(θ + βm)\n",
    "\n",
    "\n",
    "Step2 : θ <- θ + m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f424879-e798-4acf-b143-2ae421b6d4d5",
   "metadata": {},
   "source": [
    "#### How Nesterov Accelerated Gradient works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d2e77-59d4-470f-881f-08bc0aa62b6b",
   "metadata": {},
   "source": [
    "This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position, as you can see in Figure 11-6 ...refer textbook (where ∇1 represents the gradient of the cost function measured at the starting point θ, and ∇2 represents the gradient at the point located at θ + βm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c84096-e8c8-4c11-9202-49be428e9c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "724f6ab1-2d1a-42eb-bb22-a4a73938648b",
   "metadata": {},
   "source": [
    "#### Nestorov Accelerated Gradient conclusion\n",
    "As you can see, the Nesterov update ends up slightly closer to the optimum. After a while, these small improvements add up and NAG ends up being significantly faster than regular momentum optimization. Moreover, note that when the momentum pushes the weights across a valley, ∇1 continues to push farther across the valley, while ∇2 pushes back toward the bottom of the valley. This helps reduce oscillations and thus NAG converges faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1d7f3-282d-403b-b42b-2dfdca579536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db9f242-499b-4751-895a-93ae200b9ace",
   "metadata": {},
   "source": [
    "##### Nestorov Accelerated Gradient with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f907f686-7dc2-4409-8640-6073d8a52000",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf7033-45df-4279-ba6d-97ec0c85bb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c751035-f64b-4267-9627-933105a5edec",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "\n",
    "##### Warning\n",
    "AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an Adagrad optimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression, though). Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate optimizers.izers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737cf7c6-606a-4409-9ecc-d3d8854ca966",
   "metadata": {},
   "source": [
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c8f8a-9f6f-41ab-a4c6-837790db4252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa50c8e-9506-49ac-96c4-652df80748a7",
   "metadata": {},
   "source": [
    "### AdaGrad equation\n",
    "Step1 : s <- s + ∇θ.J(θ) ⊗ ∇θ.J(θ)\n",
    "\n",
    "\n",
    "Step2 : θ <- θ − η ∇θ.J(θ) ⊘ sqrt(s + ε)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc532f-8b4f-4fe3-8e17-daa0b67bc50b",
   "metadata": {},
   "source": [
    "### Explanation Adagrad equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f659ca-8513-497b-8670-6d9dd4906d50",
   "metadata": {},
   "source": [
    "#### First step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85bb29-13cb-4a39-9747-91fc9e810f7c",
   "metadata": {},
   "source": [
    "The first step accumulates the square of the gradients into the vector s (recall that the  \r\n",
    "⊗ symbol represents the element-wise multiplication). This vectorized form is equivalent to computing si ← si + (∂ J(θ) / ∂ θi)^2for each element si of the vector s; in other words, each si accumulates the squares of the partial derivative of the cost function with regard to parameter θi. If the cost function is steep along the ith dimension, then si will get larger and larger at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1dee15-c6cd-492b-bdbb-1fad505e1115",
   "metadata": {},
   "source": [
    "#### Second Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f290c-7d07-43b4-bc2f-20b2f189f584",
   "metadata": {},
   "source": [
    "The second step is almost identical to Gradient Descent,(the ⊘ symbol represents the element-wise division, and ε is a smoothing term to avoid division by zero, typically set to 10^ -10). The one big difference from Gradient descent is that the gradient vector is scaled down by a factor of  sqrt(s + ε)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532191a6-b8a1-4c65-a908-306ac6f9c2ad",
   "metadata": {},
   "source": [
    "#### Adagrad in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00a5a79e-6ae1-4fab-9024-ba5336672d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2c2b0-e179-418c-b519-f27845997b4b",
   "metadata": {},
   "source": [
    "### Adagrad Conclusive statement\n",
    "\n",
    "In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum. One additional benefit is that it requires much less tuning of the learning rate hyperparameter η."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c7ada-b933-4baf-ae4f-9c67c8a1d14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9affc88b-6f1f-4585-ab78-f54e66b41bbb",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7503009-9aef-4f17-8a08-1cf5f7a39518",
   "metadata": {},
   "source": [
    "As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never converging to the global optimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb643b-0df0-42e3-83e1-1408bd1b01d4",
   "metadata": {},
   "source": [
    "### Working RMSProp algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45ba3c-3693-4722-8872-2c184d47d229",
   "metadata": {},
   "source": [
    "The RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations as opposed to all the gradients since the beginning of training. It does so by using exponential decay in the first step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cbcec-1633-431a-b89a-84af720020b5",
   "metadata": {},
   "source": [
    "#### RMSProp Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6527a5-8022-4979-8163-fa64c73fd408",
   "metadata": {},
   "source": [
    "Equation RMSProp algorithm\n",
    "\n",
    "\r\n",
    "Step 1 s <- βs + (1 − β).∇θ.J(θ) ⊗ ∇θ.J(θ)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "Step 2 θ <- θ − η.∇θ.(Jθ) ⊘ sqrt(s + ε)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ada87e-160e-44cf-9b7c-662b3d9e1bd9",
   "metadata": {},
   "source": [
    "#### Parameter RMSProp\n",
    "The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so you may not need to tune it at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671db17e-d55c-46fe-87db-e73b873a95b1",
   "metadata": {},
   "source": [
    "#### Implementing RMSProp in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d5cf5bc-1a41-491e-92fe-1227e018a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9) #Note that the rho argument corresponds to β\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf376c0-427f-415a-b564-299bb1cd1c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f9a99a6-28b1-4287-a4cf-5fffc9bf9966",
   "metadata": {},
   "source": [
    "## Adam and Nadam Optimization  IMPORTANT!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a434ef-256c-4e5a-aad0-bf7e7a78dfa2",
   "metadata": {},
   "source": [
    "Adam,  which stands for adaptive moment estimation, combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb3852-279d-49d2-94c9-56c672f5b5a1",
   "metadata": {},
   "source": [
    "#### Equation of Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a60f32-19fb-4e82-ac1b-42e317981f30",
   "metadata": {},
   "source": [
    "Equation 11-8. Adam algorithm\r\n",
    "1) m <- β1m − (1 − β1).∇θ.J(θ)\r\n",
    "\r\n",
    "\r\n",
    "2) s <- β2.s + (1 − β2).∇θ.J(θ) ⊗ ∇θ.J(θ)\r\n",
    "\r\n",
    "\r\n",
    "3) m <- m /(1 − β1^t)\r\n",
    "\r\n",
    "\r\n",
    "4) s <- s /(1 − β2^t)\r\n",
    "\r\n",
    "\r\n",
    "5) θ <− θ + η.m ⊘ sqrt(s + ε) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b330e57-b54c-4cf9-9af0-6d3364327daf",
   "metadata": {},
   "source": [
    "#### Explanation of equation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efce2ec-ddf3-4710-8e37-15253fde427b",
   "metadata": {},
   "source": [
    "In this equation, t represents the iteration number (starting at 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccb92e-55d8-411a-9695-f2b2edf4dc5a",
   "metadata": {},
   "source": [
    "##### STEPS 1,2 and 5\n",
    "\n",
    "If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity to both momentum optimization and RMSProp. The only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but these are actually equivalent except for a constant factor (the decaying average is just 1 – β1 times the decaying sum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f88864-37e7-4dd3-b83f-11b536cb025c",
   "metadata": {},
   "source": [
    "##### STEPS 3 and 4\n",
    "\n",
    "Steps 3 and 4 are somewhat of a technical detail: since m and s are initialized at 0, they will be biased toward 0 at the beginning of training, so these two steps will help boost m and s at the beginning of training and reduce the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a477c-9bd3-49e2-a49b-5fce8df01e8a",
   "metadata": {},
   "source": [
    "##### The hyperparameters\n",
    "The momentum decay hyperparameter β1 is typically initialized to 0.9, while the scaling decay hyperparameter β2 is often initialized to 0.999. As earlier, the smoothing term ε is usually initialized to a tiny number such as 10^–7. These are the default values for the Adam class (to be precise, epsilon defaults to None, which tells Keras to use keras.backend.epsilon(), which defaults to 10^–7; you can change it using keras.backend.set_epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8439ca0-10a3-4886-81ea-2d7a8e7652af",
   "metadata": {},
   "source": [
    "#### Implementing Adam in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e771ea35-755e-4115-95e0-a37604849640",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0b1f7-122f-44ff-83ea-afc1fb97f902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24a7f784-c0a8-40bf-b2c1-1c719de0c80d",
   "metadata": {},
   "source": [
    "### Variants of ADAM optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef01c6-e78c-45be-8995-c1f9c2193b58",
   "metadata": {},
   "source": [
    "### AdaMax\n",
    "Notice that in step 2 of Adam optimizer Equation , Adam accumulates the squares of the gradients in s (with a greater weight for more recent gradients). In step 5, if we ignore ε and steps 3 and 4 (which are technical details anyway), Adam scales down the parameter updates by the square root of s. In short, Adam scales down the parameter updates by the ℓ2 norm of the time-decayed gradients (recall that the ℓ2 norm is the square root of the sum of squares, and here the vector s accumulates the square of the gradients). AdaMax, introduced in the same paper as Adam, replaces the ℓ2 norm with the ℓ∞ norm (a fancy way of saying the max). Specifically, it replaces step 2 in the Adam optimizer Equation with s ← max(β2s,∇θJ(θ)), it drops step 4, and in step 5 it scales down the gradient updates by a factor of s, which is just the max of the time-decayed gradients. In practice, this can make AdaMax more stable than Adam, but it really depends on the dataset, and in general Adam performs better. So, this is just one more optimizer you can try if you experience problems with Adam on some task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb53c2a-398e-43d0-b545-956907bae616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3403643a-9608-4400-b709-aae5c964a71a",
   "metadata": {},
   "source": [
    "### Nadam\n",
    "Nadam optimization is Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam. In his report introducing this technique, the researcher Timothy Dozat compares many different optimizers on various tasks and finds that Nadam generally outperforms Adam but is sometimes outperformed by RMSProp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87e20f-aafa-4919-964a-8d01d43eb401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b8c5155-3928-4e3e-949b-9eae638cd882",
   "metadata": {},
   "source": [
    "## Important Note about Adaptive optimizers!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a75a1-b452-4790-a4d8-8045deab42d9",
   "metadata": {},
   "source": [
    "Adaptive optimization methods (including RMSProp, Adam, and Nadam optimization) are often great, converging fast to a good solution. However, a 2017 paper20 by Ashia C. Wilson et al. showed that they can lead to solutions that generalize poorly on some datasets. So when you are disappointed by your model’s performance, try using plain Nesterov Accelerated Gradient instead: your dataset may just be allergic to adaptive gradients. Also check out the latest research, because it’s moving fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec86f68-80d6-47e2-873a-4a0a21c48c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa5ff71e-d74c-4616-aa06-db8e57cc0117",
   "metadata": {},
   "source": [
    "## Second Order Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ab363-c32c-45b1-85c8-148e0f746cca",
   "metadata": {},
   "source": [
    "All the optimization techniques discussed so far only rely on the first-order partial derivatives (Jacobians). The optimization literature also contains amazing algorithms based on the second-order partial derivatives (the Hessians, which are the partial derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are n 2 Hessians per output (where n is the number of parameters), as opposed to just n Jacobians per output. Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the Hessians is just too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad54664-35ce-4a01-93d2-1dcdd77c0ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12341694-73e7-4aab-aa76-89b2fb3233a9",
   "metadata": {},
   "source": [
    "### Pro-Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61382c11-9999-420a-872c-9548260e9bfc",
   "metadata": {},
   "source": [
    "Check out the TensorFlow Model Optimization Toolkit (TF-MOT), which provides a pruning API capable of iteratively removing connections during training based on their magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21063757-cbe5-4d04-a499-7eaac91fbc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5be7cf4-d554-44d4-b807-196c2db52e91",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184114d-7866-4c23-9478-d05abfef742d",
   "metadata": {},
   "source": [
    "Finding a good learning rate is very important. If you set it much too high, training may diverge (as we discussed in “Gradient Descent” on page 118). If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never really settling down. If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488d956-db83-4b8d-b977-ec5115d09f25",
   "metadata": {},
   "source": [
    "As we discussed in Chapter 10, you can find a good learning rate by training the model for a few hundred iterations, exponentially increasing the learning rate from a very small value to a very large value, and then looking at the learning curve and picking a learning rate slightly lower than the one at which the learning curve starts shooting back up. You can then reinitialize your model and train it with that learning rate.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c718ba3-0f6a-41af-9d85-5a391463b4b6",
   "metadata": {},
   "source": [
    "But you can do better than a constant learning rate: if you start with a large learning rate and then reduce it once training stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many different strategies to reduce the learning rate during training. It can also be beneficial to start with a low learning rate, increase it, then drop it again. These strategies are called learning schedules (we briefly introduced this concept in Chapter 4). These are the most commonly used learning schedules:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb52a3-73f5-46bb-a80c-af61b21fffd0",
   "metadata": {},
   "source": [
    "### Different Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64a967-530e-4535-bc96-fcafaec2c225",
   "metadata": {},
   "source": [
    "### Power scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12206155-3ec1-461c-b440-cc4f1beae883",
   "metadata": {},
   "source": [
    "Set the learning rate to a function of the iteration number t, i.e. η(t) = η0 / (1 + t/s)^c.The initial learning rate η0 , the power c (typically set to 1), and the steps s are hyperparameters. The learning rate drops at each step. After s steps, it is down to η0/2. After s more steps, it is down to η0/3, then it goes down to η0/4, then η0/5, and so on. As you can see, this schedule first drops quickly, then more and more slowly. Of course, power scheduling requires tuning η0 and s (and possibly c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f65c30-0123-4565-89af-56476cf35b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58d93134-1f6b-45d6-b174-a7c645a9f5bc",
   "metadata": {},
   "source": [
    "### Exponential scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ad281-91b6-4ea9-9394-a27b62841d37",
   "metadata": {},
   "source": [
    "Set the learning rate to η(t) = η0 x 0.1^(t/s). The learning rate will gradually drop by a factor of 10 every s steps. While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every s steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef15430-6fc1-4b23-928b-4562d736c596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaa28ab4-434c-4901-bd2b-220a0437e1da",
   "metadata": {},
   "source": [
    "### Piecewise constant scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5be13e-6d7f-44cb-8754-2f3544574ff5",
   "metadata": {},
   "source": [
    "Use a constant learning rate for a number of epochs (e.g., η0 = 0.1 for 5 epochs), then a smaller learning rate for another number of epochs (e.g., η1 = 0.001 for 50 epochs), and so on. Although this solution can work very well, it requires fiddling around to figure out the right sequence of learning rates and how long to use each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a5046-bfc2-43e1-8265-4c6abb7da1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "586b0ce9-6309-4e13-ba7f-a6759a6207bb",
   "metadata": {},
   "source": [
    "### Performance scheduling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084bc85a-b8ba-40bb-9959-f4b472880dc5",
   "metadata": {},
   "source": [
    "Measure the validation error every N steps (just like for early stopping), and\r\n",
    "reduce the learning rate by a factor of λ when the error stops dropping.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d0047-c272-4d88-b482-b50f2bfc251f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6cd5051-e67d-4495-a10d-7c8f3940bc74",
   "metadata": {},
   "source": [
    "### 1 cycle scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202075c8-1c88-4571-accb-23076a4fd505",
   "metadata": {},
   "source": [
    "Contrary to the other approaches, 1cycle (introduced in a 2018 paper21 by Leslie Smith) starts by increasing the initial learning rate η0, growing linearly up to η1 halfway through training. Then it decreases the learning rate linearly down to η0 again during the second half of training, finishing the last few epochs by dropping the rate down by several orders of magnitude (still linearly). The maximum learning rate η1 is chosen using the same approach we used to find the optimal learning rate, and the initial learning rate η0 is chosen to be roughly 10 times lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81069867-0a63-4dbf-99f2-d29d14ee3ed6",
   "metadata": {},
   "source": [
    "#### 1 cycle approach with momentum optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5656ff96-aef7-4248-936f-8f2058e5ca57",
   "metadata": {},
   "source": [
    "When using a momentum, we start with a high momentum first (e.g.,0.95), then drop it down to a lower momentum during the first half of training (e.g., down to 0.85, linearly), and then bring it back up to the maximum value (e.g., 0.95) during the second half of training, finishing the last few epochs with that maximum value. Smith did many experiments showing that this approach was often able to speed up training considerably and reach better performance.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0965bc-56fa-43cf-a5a3-090f50a6c14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bcf058d-9eb9-4d62-b9db-22a405a6f137",
   "metadata": {},
   "source": [
    "### Research results on learning rate schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271cb69f-0901-48ee-ac2c-0b1eb2052f1d",
   "metadata": {},
   "source": [
    "A 2013 paper22 by Andrew Senior et al. compared the performance of some of the most popular learning schedules when using momentum optimization to train deep neural networks for speech recognition. The authors concluded that, in this setting, both performance scheduling and exponential scheduling performed well. They favored exponential scheduling because it was easy to tune and it converged slightly faster to the optimal solution (they also mentioned that it was easier to implement than performance scheduling, but in Keras both options are easy). That said, the 1cycle approach seems to perform even better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4dec88-dee5-42fe-94e5-4a2bca608f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a656bbc3-9c87-4f0a-9666-79f655cca0f4",
   "metadata": {},
   "source": [
    "## Implementing Learning rate schedulers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04dd675-009e-425a-a192-03ea33393ebf",
   "metadata": {},
   "source": [
    "### Power Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5091eb-37a7-403f-bede-7a79b158ffa2",
   "metadata": {},
   "source": [
    "Implementing power scheduling in Keras is the easiest option: just set the decay hyperparameter when creating an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9feb2889-2b9c-477d-afae-5286f2450b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.legacy.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d688ef2-5b49-471f-8d94-426d118c7cc0",
   "metadata": {},
   "source": [
    "The decay is the inverse of s (the number of steps it takes to divide the learning rate by one more unit), and Keras assumes that c is equal to 1.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0febba-5ece-440e-bb49-44b0325ea083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6dae011-48a8-49c0-a4e9-f60c359e3d5a",
   "metadata": {},
   "source": [
    "### Exponential Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf4a5b-584f-4006-ac0f-8153198e5e59",
   "metadata": {},
   "source": [
    " You first need to define a function that takes the current epoch and returns the learning rate. For example, let’s implement exponential scheduling:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e482218-b702-47aa-8dc5-37814912f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa40b9-5618-4d22-9191-be9a03538256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "252ff463-cb43-4c41-bd77-e4ddcc9074ae",
   "metadata": {},
   "source": [
    "If you do not want to hardcode η0 and s, you can create a function that returns a configured function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1a61a23-b2f7-4089-8eb7-93f5c31068e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "466ea5a0-7227-4710-8df8-04c5e26bb1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663aecd2-1b2b-4e4b-a303-c48d242a08a3",
   "metadata": {},
   "source": [
    "Next, create a LearningRateScheduler callback, giving it the schedule function, and \n",
    "pass this callback to the fit() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78d2de8d-f532-4d32-a029-172db8a29e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eebcda-3fd5-435d-8038-f0260267e1a5",
   "metadata": {},
   "source": [
    "#### Exponential Scheduling working\n",
    "The LearningRateScheduler will update the optimizer’s learning_rate attribute at the beginning of each epoch. Updating the learning rate once per epoch is usually enough, but if you want it to be updated more often, for example at every step(here step is that the gradient descent algorithm takes on the curve to reach the global optimum, or, you can simply define step to be equal to the number of epochs multiplied by len(x_train),i.e. steps = 20 x len(x_train) ), you can always write your own callback. Updating the learning rate at every step makes sense if there are many steps per epoch. Alternatively, you can use the keras.optimizers.schedules approach, described shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3194a26-fd59-414a-9efc-4263dc9d49c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b150ca4-c417-4d63-b521-369a647e489a",
   "metadata": {},
   "source": [
    "#### Starting the Exponential decay at the beginning i.e. at epoch 0 and not at epoch 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a524bf26-cc37-4273-a384-4b86ad30044b",
   "metadata": {},
   "source": [
    "The schedule function can optionally take the current learning rate as a second argument. For example, the following schedule function multiplies the previous learning rate by (0.1)^1/20, which results in the same exponential decay (except the decay now starts at the beginning of epoch 0 instead of 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a54207f3-e4a0-40af-bd17-c38e5e8f913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b7352-77cd-4147-bf26-e2a092682060",
   "metadata": {},
   "source": [
    "#### Note about the exponential decay at epoch 0\n",
    "\n",
    "This implementation relies on the optimizer’s initial learning rate contrary to the previous implementation, which means that in previous implementation, first, the first epoch happens, and then the learning rate gets updated by the algorithm, however here we are updating it even before the training starts SO MAKE SURE TO SET IT APPROPRIATELY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0085527-a754-412c-9d2c-3f3f175dc779",
   "metadata": {},
   "source": [
    "#### Note about the epoch parameter\n",
    "When you save a model, the optimizer and its learning rate get saved along with it. This means that with this new schedule function, you could just load a trained model and continue training where it left off, no problem. Things are not so simple if your schedule function uses the epoch argument, however: the epoch does not get saved, and it gets reset to 0 every time you call the fit() method. If you were to continue training a model where it left off, this could lead to a very large learning rate, which would likely damage your model’s weights. One solution is to manually set the fit() method’s initial_epoch argument so the epoch starts at the right value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388c317-bd33-491b-94d7-f07ca5391742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee978090-0fcc-43de-884d-6a91edf3805d",
   "metadata": {},
   "source": [
    "### Piecewise constant scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85d084-a792-4f52-b395-27eb01b77081",
   "metadata": {},
   "source": [
    "you can use a schedule function like the following one (as earlier, you can define a more general function if you want; see the “Piecewise Constant Scheduling” section of the notebook for an example), then create a LearningRateScheduler callback with this function and pass it to the fit() method, just like we did for exponential scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b5e956b-1754-43dc-904a-d9594fb1487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4569ba97-48a0-4ac3-8815-fc0164ace330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62e82eb0-c62e-49e5-a1bb-07e4be1ff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f9e30-3ce1-41ae-a198-630248c5ccf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "223ce0cc-e217-4877-af01-acff274ec69f",
   "metadata": {},
   "source": [
    "### Performance Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f6603-655d-48a4-b61a-b815013d8ba9",
   "metadata": {},
   "source": [
    "For performance scheduling, use the ReduceLROnPlateau callback. For example, if you pass the following callback to the fit() method, it will multiply the learning rate by 0.5 whenever the best validation loss does not improve for five consecutive epochs (other options are available; please check the documentation for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "143b3b3f-c419-40b9-91ad-9d4d019d22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5105d8b-e1e5-4ea7-966f-efb818d89a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb035fd1-d7ed-43ab-b87f-5b8cac1b13f8",
   "metadata": {},
   "source": [
    "### keras.optimizers.schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce596c-978d-46c6-aeb5-2c18e0d07d71",
   "metadata": {},
   "source": [
    "Lastly, tf.keras offers an alternative way to implement learning rate scheduling: define the learning rate using one of the schedules available in keras.optimizers.schedules, then pass this learning rate to any optimizer. This approach updates the learning rate at each step rather than at each epoch. For example, here is how to implement the same exponential schedule as the exponential_decay_fn() function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d70cc5a-b430-4539-8c22-10ad4dbb5a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "637922c4-57b9-4883-9a9a-25100fc789e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a80d8e8-64d0-4d19-803c-f386d24dd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365eaad-411e-44be-a9eb-e1078882f58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc155263-c868-450f-a8a7-cc089ea3ad67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a681f46-d396-47ed-927b-9af819571e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 20 * len(X_train_full) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737fc76-c48d-4945-a401-3078c13e341a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8ef1ace-8530-4186-940e-77577ef235af",
   "metadata": {},
   "source": [
    "### 1 Cycle approach, please refer author's jupyter notebook!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cbcfdc-81d7-454e-9031-7f3c993562d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08750a4-c418-46f1-8d21-fd2ba6f52046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d1ee5-5e24-4562-86a4-006d0ea9c925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "341998c9-0135-4c12-b3a9-daa923c70160",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18eb690-3e22-450a-b6a7-c454bd4be60e",
   "metadata": {},
   "source": [
    "Deep neural networks typi‐cally have tens of thousands of parameters, sometimes even millions. This gives them an incredible amount of freedom and means they can fit a huge variety of complex datasets. But this great flexibility also makes the network prone to overfitting the training set. We need regularization.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe3212-f8b6-4b8b-91fc-9052d55cc8e6",
   "metadata": {},
   "source": [
    "In this section we will examine other popular regularization techniques for neural networks: ℓ1 and ℓ2 regularization, dropout, and max-norm regularization.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70239172-17ca-484d-9348-83ee65b241b5",
   "metadata": {},
   "source": [
    "### ℓ1 and ℓ2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df87613-5c2b-4e88-a625-1ba3b3eb0119",
   "metadata": {},
   "source": [
    "Just like you did in Chapter 4 for simple linear models, you can use ℓ2 regularization to constrain a neural network’s connection weights, and/or ℓ1 regularization if you want a sparse model (with many weights equal to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c7b33-d6a5-4dcf-b7e6-e835af6d22bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f76567bd-c837-4567-8079-e06d751b6c7e",
   "metadata": {},
   "source": [
    "### ℓ2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a00a664a-04e0-4f7e-9a40-b26f031b4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948848f-ae73-4da3-b506-086297f6f25f",
   "metadata": {},
   "source": [
    "The l2() function returns a regularizer that will be called at each step during training to compute the regularization loss. This is then added to the final loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9774e-893d-4b89-ba16-dceb28670769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6259e87-5b90-466d-b90f-eea5b8d06be6",
   "metadata": {},
   "source": [
    "### ℓ1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f97b1-869f-40b5-b898-1655303ae865",
   "metadata": {},
   "source": [
    "As you might expect, you can just use keras.regularizers.l1() if you want ℓ1 regularization; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae8d0765-fcf0-4e1c-8af3-be70458b9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l1(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b0cb4e-17cf-4bde-894f-e568503d28a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d17600d1-439c-4b77-8022-a7ee6302b465",
   "metadata": {},
   "source": [
    "### ℓ1 and ℓ2 Regularization together keras implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0019d652-cc0b-4d64-b417-8d69ff9aadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l1_l2(0.01) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da53acb-9a2e-4b2d-9f91-c93e9fb8a7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee5a153f-9862-48d8-a983-1ae479069555",
   "metadata": {},
   "source": [
    "### Pro-Tip Writing neat code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd336b82-471f-4ea6-afc9-5e2c67669b0f",
   "metadata": {},
   "source": [
    "Since you will typically want to apply the same regularizer to all layers in your network, as well as using the same activation function and the same initialization strategy in all hidden layers, you may find yourself repeating the same arguments. This makes the code ugly and error-prone. To avoid this, you can try refactoring your code to use loops. Another option is to use Python’s functools.partial() function, which lets you create a thin wrapper for any callable, with some default argument values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10031163-dd9c-490a-8a9f-c727a23e3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7950c194-a1e9-4bf3-9922-8d4389bb4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RegularizedDense = partial(keras.layers.Dense, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "675483b4-7d5f-4dda-8530-e3afaf361745",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape=[28, 28]),\n",
    " RegularizedDense(300),\n",
    " RegularizedDense(100),\n",
    " RegularizedDense(10, activation=\"softmax\",\n",
    " kernel_initializer=\"glorot_uniform\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66702692-feef-439d-8666-f6efcac56609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ed0ee90-b15e-4ec4-b662-d60f88ee2338",
   "metadata": {},
   "source": [
    "## Dropout IMPORTANT!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff39b97-1de8-4bbe-aa89-f16b2f23d078",
   "metadata": {},
   "source": [
    "Dropout is one of the most popular regularization techniques for deep neural networks. It was proposed in a paper23 by Geoffrey Hinton in 2012 and further detailed in a 2014 paper24 by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-the-art neural networks get a 1–2% accuracy boost simply by adding dropout. This may not sound like a lot, but when a model already has 95% accuracy, getting a 2% accuracy boost means dropping the error rate by almost 40% (going from 5% error to roughly 3%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c959d6-8867-421f-8a3e-23bdfe111079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ade8b48a-821e-40c5-a558-e1b6c06b5517",
   "metadata": {},
   "source": [
    "### Working of Dropout Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f817e7-3056-4df2-8c3b-7f982b88e451",
   "metadata": {},
   "source": [
    "It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step . The hyperparameter p is called the dropout rate, and it is typically set between 10% and 50%: closer to 20-30% in recurrent neural nets, and closer to 40–50% in convolutional neural networks. After training, neurons don’t get dropped anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538cecc-b84a-445a-ac3d-4d2afbb95859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de7095ad-bfc3-43b7-9b06-5a1c66532d47",
   "metadata": {},
   "source": [
    "#### Analogy working of Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85b6ec-7c30-4616-b406-7873aedeb738",
   "metadata": {},
   "source": [
    "It’s surprising at first that this destructive technique works at all. Would a company perform better if its employees were told to toss a coin every morning to decide whether or not to go to work? Well, who knows; perhaps it would! The company would be forced to adapt its organization; it could not rely on any single person to work the coffee machine or perform any other critical tasks, so this expertise would have to be spread across several people. Employees would have to learn to cooperate with many of their coworkers, not just a handful of them. The company would become much more resilient. If one person quit, it wouldn’t make much of a difference. It’s unclear whether this idea would actually work for companies, but it certainly does for neural networks. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ce268-b57d-4f2b-bbc0-8cef185a4cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22715d0c-2ab2-4c7a-8484-0ff15aafbad0",
   "metadata": {},
   "source": [
    "#### Technical explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956cd5d2-50e2-46c2-b875-a0cfcded522d",
   "metadata": {},
   "source": [
    "Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there are a total of 2N possible networks (where N is the total number of droppable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice. Once you have run 10,000 training steps, you have essentially trained 10,000 different neural networks (each with just one training instance). These neural networks are obviously not independent because they share many of their weights, but they are nevertheless all different. The resulting neural network can be seen as an averaging ensemble of all these smaller neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3ad76-1830-4b28-9377-3e4c80de0231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a58f3843-eff5-4728-b641-ee5a3ee5bf54",
   "metadata": {},
   "source": [
    "#### Pro-Tip\n",
    "In practice, you can usually apply dropout only to the neurons in the top one to three layers (excluding the output layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36018d51-42fa-4a6c-ba67-1d44411104a5",
   "metadata": {},
   "source": [
    "### IMPORTANT TECHNICAL DETAIL DROPOUT!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba0e46-2e12-4bd8-ac94-cec3d082c3a3",
   "metadata": {},
   "source": [
    "There is one small but important technical detail. Suppose p = 50%, in which case during testing a neuron would be connected to twice as many input neurons as it would be (on average) during training. To compensate for this fact, we need to divide each neuron’s input connection weights by 2 after training. If we don’t, each neuron will get a total input signal roughly twice as large as what the network was trained on and will be unlikely to perform well, this basically means that earlier let's say there were 20 neurons in the first layer and 10 in the second layer, so with a dropout of 50%, a neuron in the second layer was only getting connections from just 10 neuron in the first layer, and hence it's weights were adjusted in that way, but during training all 20 neurons will be active hence it will get twice the signal than it did during the training. More generally, we need to multiply each input connection weight by the keep probability (1 – p) after training. Alternatively, we can divide each neuron’s output by the keep probability during training (these alternatives are not perfectly equivalent, but they work equally well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decfb43-8718-4744-a271-c6513370ce0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fffb760d-322a-4808-8da0-038b08686550",
   "metadata": {},
   "source": [
    "### Implementing Dropout in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9b680-5ecc-4da0-b66b-f7c4378b9bd7",
   "metadata": {},
   "source": [
    "To implement dropout using Keras, you can use the keras.layers.Dropout layer. During training, it randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability. After training, it does nothing at all; it just passes the inputs to the next layer. The following code applies dropout regularization before every Dense layer, using a dropout rate of 0.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d9b299e-46d0-410c-8b6a-cc370950f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape=[28, 28]),\n",
    " keras.layers.Dropout(rate=0.2),\n",
    " keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    " keras.layers.Dropout(rate=0.2),\n",
    " keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    " keras.layers.Dropout(rate=0.2),\n",
    " keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f71831-ae37-4ea5-a6d6-d376e4b4e089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfc808d0-bdf7-44b7-8cc6-6f46e8265c9e",
   "metadata": {},
   "source": [
    "#### Error Comparison with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74e119-1561-4c59-bbfa-58cc36235420",
   "metadata": {},
   "source": [
    "Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f7f40-fecc-4a66-a556-672e7bf6aa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12bbd716-18bb-4e1b-8b0b-619d5c6f4188",
   "metadata": {},
   "source": [
    "#### Updating Dropout as per model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649d8cd-5942-4a75-9780-26a1958f7821",
   "metadata": {},
   "source": [
    "If you observe that the model is overfitting, you can increase the dropout rate. Conversely, you should try decreasing the dropout rate if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones. Moreover, many state-of-the-art architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong. Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df371c4d-635c-4320-8540-d6b12b955aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c70afd8-7652-4473-94c6-3c59a0bd4c79",
   "metadata": {},
   "source": [
    "#### Self-Normalizing and Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16815a-a53e-48b6-a03b-2a52092f9f82",
   "metadata": {},
   "source": [
    "If you want to regularize a self-normalizing network based on the SELU activation function (as discussed earlier), you should use alpha dropout: this is a variant of dropout that preserves the mean and standard deviation of its inputs (it was introduced in the same paper as SELU, as regular dropout would break self-normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0bbfc-22c7-4430-adb4-96bc65dcdae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f079d2-c991-4466-af43-3a37781d0f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cec0204-2f77-480d-9fe8-a8b0ad88916f",
   "metadata": {},
   "source": [
    "## Monte Carlo (MC) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0339d-a021-43da-8170-e6bd9b92ada6",
   "metadata": {},
   "source": [
    "In 2016, a paper by Yarin Gal and Zoubin Ghahramani added a few more good reasons to use dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b9da9-bcb2-424c-9a04-d6125c680da5",
   "metadata": {},
   "source": [
    "1) First, the paper established a profound connection between dropout networks (i.e., neural networks containing a Dropout layer before every weight layer) and approximate Bayesian inference(Bayesian inference is a method of statistical inference in which Bayes' theorem is used to calculate a probability of a hypothesis, given prior evidence, and update it as more information becomes available.), Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian \n",
    "inference in a specific type of probabilistic model called a Deep Gaussian Process. giving dropout a solid mathematical justification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458411c-8dd8-415d-9960-2fe946bd642e",
   "metadata": {},
   "source": [
    "2) Second, the authors introduced a powerful technique called MC Dropout, which can boost the performance of any trained dropout model without having to retrain it or even modify it at all, provides a much better measure of the model’s uncertainty, and is also amazingly simple to implement.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1a492c6-f540-432c-b688-9a5f0f4f98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4a9b8eb-e143-4cb4-a410-9653bdb2903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c1a2c37-6092-4f3b-ab9b-a1926c09aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51a124a8-5c2b-4d85-ac7e-99f674bfb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90c15b96-bf93-4a68-be5b-c984af1d4631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Python 3.11\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Python 3.11\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6686 - accuracy: 0.7571 - val_loss: 0.6170 - val_accuracy: 0.8378\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5574 - accuracy: 0.7945 - val_loss: 0.5655 - val_accuracy: 0.8470\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5282 - accuracy: 0.8044 - val_loss: 0.5118 - val_accuracy: 0.8514\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5067 - accuracy: 0.8137 - val_loss: 0.5036 - val_accuracy: 0.8472\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4945 - accuracy: 0.8164 - val_loss: 0.5000 - val_accuracy: 0.8430\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4803 - accuracy: 0.8222 - val_loss: 0.5112 - val_accuracy: 0.8466\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4754 - accuracy: 0.8238 - val_loss: 0.4531 - val_accuracy: 0.8664\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4656 - accuracy: 0.8267 - val_loss: 0.5076 - val_accuracy: 0.8586\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4576 - accuracy: 0.8304 - val_loss: 0.4793 - val_accuracy: 0.8554\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4532 - accuracy: 0.8311 - val_loss: 0.4627 - val_accuracy: 0.8590\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4446 - accuracy: 0.8345 - val_loss: 0.4249 - val_accuracy: 0.8762\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4409 - accuracy: 0.8369 - val_loss: 0.4226 - val_accuracy: 0.8744\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4385 - accuracy: 0.8365 - val_loss: 0.4365 - val_accuracy: 0.8754\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4349 - accuracy: 0.8391 - val_loss: 0.4650 - val_accuracy: 0.8662\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4340 - accuracy: 0.8402 - val_loss: 0.4789 - val_accuracy: 0.8714\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4255 - accuracy: 0.8407 - val_loss: 0.4362 - val_accuracy: 0.8758\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4249 - accuracy: 0.8422 - val_loss: 0.4277 - val_accuracy: 0.8724\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4216 - accuracy: 0.8439 - val_loss: 0.4075 - val_accuracy: 0.8798\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4230 - accuracy: 0.8423 - val_loss: 0.4134 - val_accuracy: 0.8784\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4202 - accuracy: 0.8445 - val_loss: 0.4145 - val_accuracy: 0.8796\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cace05b3-af37-4d88-b1c0-44194b4ae7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298d54a-1783-4947-b9a5-b5998e16e18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "657757e6-b2a6-4fb0-944f-fe711677ac75",
   "metadata": {},
   "source": [
    "### Implementing MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "588f8b42-b8ee-4c64-b363-3a3a9ec5f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)]) # This model(X_test_scaled, training=True) is same as calling model.predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7237f5f-a248-4140-ac58-7166b8c33fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.03501263e-10, 3.71479159e-09, 5.05451041e-13, ...,\n",
       "         6.32154522e-03, 2.50838150e-09, 9.83320415e-01],\n",
       "        [4.02352634e-05, 6.55955503e-08, 9.93498802e-01, ...,\n",
       "         3.29885802e-06, 7.15154329e-07, 2.06066488e-06],\n",
       "        [5.81921098e-17, 1.00000000e+00, 2.96844392e-18, ...,\n",
       "         1.04566007e-28, 1.38250864e-20, 3.05177215e-23],\n",
       "        ...,\n",
       "        [1.13408292e-04, 8.10023812e-06, 1.97748144e-04, ...,\n",
       "         1.21452831e-05, 9.96912479e-01, 1.20567404e-06],\n",
       "        [2.33907826e-09, 9.99999523e-01, 2.44983694e-13, ...,\n",
       "         6.33552932e-12, 1.43895078e-11, 2.81225843e-09],\n",
       "        [6.80977319e-05, 8.56973784e-05, 5.01084840e-04, ...,\n",
       "         1.65345922e-01, 7.53034186e-03, 9.39171389e-03]],\n",
       "\n",
       "       [[4.00161067e-12, 3.91765242e-09, 2.48228711e-13, ...,\n",
       "         1.86255500e-01, 2.42334846e-08, 7.85112262e-01],\n",
       "        [1.05568215e-05, 7.72399478e-09, 9.97696579e-01, ...,\n",
       "         6.55009103e-10, 1.46341364e-07, 3.28073502e-09],\n",
       "        [1.98312880e-10, 1.00000000e+00, 9.36500226e-13, ...,\n",
       "         1.36215273e-16, 7.77513362e-14, 6.11650325e-13],\n",
       "        ...,\n",
       "        [1.86904272e-05, 3.88656008e-06, 1.20377285e-06, ...,\n",
       "         1.99591814e-05, 9.99879599e-01, 1.02730610e-06],\n",
       "        [1.91218902e-07, 9.99970198e-01, 2.61946784e-08, ...,\n",
       "         1.14444596e-08, 1.99852948e-05, 1.03134887e-07],\n",
       "        [2.60714533e-10, 3.08442660e-10, 4.08938786e-11, ...,\n",
       "         8.34153414e-01, 7.05331058e-06, 7.00683938e-03]],\n",
       "\n",
       "       [[2.76092705e-05, 2.60769848e-05, 1.60534946e-05, ...,\n",
       "         2.31338963e-01, 2.46081277e-06, 6.07075334e-01],\n",
       "        [2.17286724e-04, 5.79396975e-10, 9.94313359e-01, ...,\n",
       "         3.46442135e-07, 6.28148118e-05, 6.60730093e-09],\n",
       "        [1.04362016e-12, 1.00000000e+00, 1.63291012e-16, ...,\n",
       "         5.62727300e-19, 3.68149548e-13, 3.11985475e-16],\n",
       "        ...,\n",
       "        [2.30970429e-07, 8.47030890e-08, 3.11599315e-08, ...,\n",
       "         5.19899118e-07, 9.99997258e-01, 4.51358551e-09],\n",
       "        [4.36755687e-10, 9.99998093e-01, 2.52511206e-10, ...,\n",
       "         1.57347171e-13, 6.80748166e-11, 1.53682589e-09],\n",
       "        [1.80303905e-09, 1.20610588e-09, 4.15521395e-09, ...,\n",
       "         5.11496775e-02, 7.94407697e-07, 8.15929100e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.78419106e-13, 2.11832218e-10, 2.74519842e-15, ...,\n",
       "         4.41424809e-02, 6.85820107e-12, 9.55594480e-01],\n",
       "        [4.13637354e-05, 2.93986258e-09, 9.94683325e-01, ...,\n",
       "         5.22174901e-07, 1.23388460e-07, 3.23649658e-07],\n",
       "        [1.20288657e-09, 1.00000000e+00, 4.21778145e-13, ...,\n",
       "         1.20493459e-13, 7.86292744e-18, 2.31194838e-12],\n",
       "        ...,\n",
       "        [1.57354603e-04, 2.83860032e-07, 1.38188070e-05, ...,\n",
       "         1.34676350e-06, 9.99769986e-01, 3.03910724e-07],\n",
       "        [2.69048485e-07, 9.99877930e-01, 4.16924131e-08, ...,\n",
       "         1.30223896e-07, 2.99391019e-08, 8.60417185e-06],\n",
       "        [9.90341906e-11, 4.27614832e-09, 1.16372058e-10, ...,\n",
       "         8.36412534e-02, 7.83106032e-07, 2.42732316e-02]],\n",
       "\n",
       "       [[2.91778974e-08, 2.36517863e-07, 1.06567054e-10, ...,\n",
       "         1.08294422e-02, 4.03095868e-07, 9.25532758e-01],\n",
       "        [1.32952282e-05, 3.93007266e-10, 9.97168958e-01, ...,\n",
       "         6.20804883e-08, 2.96678149e-08, 1.02483089e-08],\n",
       "        [1.85180003e-14, 1.00000000e+00, 8.26940482e-18, ...,\n",
       "         2.93081360e-21, 1.79486520e-16, 4.25952355e-16],\n",
       "        ...,\n",
       "        [1.37206398e-05, 4.37977974e-08, 3.46894439e-07, ...,\n",
       "         4.29128377e-08, 9.99984026e-01, 3.78087393e-08],\n",
       "        [6.01115602e-09, 9.99973893e-01, 1.27951716e-09, ...,\n",
       "         2.35388931e-10, 2.82289658e-09, 4.60471574e-06],\n",
       "        [1.07750955e-08, 3.12038964e-07, 4.01410034e-08, ...,\n",
       "         8.77074242e-01, 2.86618154e-03, 3.43822455e-03]],\n",
       "\n",
       "       [[2.55904083e-11, 6.35898001e-09, 4.85453366e-13, ...,\n",
       "         6.22894056e-02, 9.36574290e-11, 9.37399387e-01],\n",
       "        [2.66314950e-04, 1.70368653e-09, 9.99374807e-01, ...,\n",
       "         1.83631865e-09, 5.46352658e-06, 4.38105746e-10],\n",
       "        [1.96351369e-16, 1.00000000e+00, 4.03778490e-20, ...,\n",
       "         1.03518396e-25, 5.62813689e-17, 1.86904768e-19],\n",
       "        ...,\n",
       "        [3.03843711e-03, 1.50154128e-05, 3.99826560e-04, ...,\n",
       "         1.16071344e-04, 9.94568825e-01, 2.87945441e-05],\n",
       "        [7.11951270e-06, 9.99205291e-01, 3.39281287e-05, ...,\n",
       "         1.01841913e-08, 8.05350524e-08, 1.56772285e-06],\n",
       "        [2.65969902e-07, 4.26237466e-06, 2.56567851e-08, ...,\n",
       "         3.30700189e-01, 1.21689582e-05, 2.45619360e-02]]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "023d9262-4a2d-4a18-8008-69654b42e3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a5d813b-0ce4-4445-b926-756e3d4ddb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = y_probas.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00bf74-2d99-41a5-8e89-5daaa84fec15",
   "metadata": {},
   "source": [
    "### Explanation of the above implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ecbe11-e7af-409a-bc70-2085a4951347",
   "metadata": {},
   "source": [
    "We just make 100 predictions over the test set, setting training=True to ensure that the Dropout layer is active, and stack the predictions. Since dropout is active, all the predictions will be different. Recall that predict(), (here we directly used model(x_test) which is same as model.predict()) returns a matrix with one row per instance and one column per class. Because there are 10,000 instances in the test set and 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such matrices, so y_probas is an array of shape [100, 10000, 10]. Once we average over the first dimension (axis=0), we get y_proba, an array of shape [10000, 10], like we would get with a single prediction. That’s all! Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de79b8-255f-4175-ae2a-97d89f3e7279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd3470da-7fc7-46f2-ae66-f35e397f723a",
   "metadata": {},
   "source": [
    "### Validating Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1c4e6-2b2e-44c5-acba-4ef663a4a5b3",
   "metadata": {},
   "source": [
    "let’s look at the model’s prediction for the first instance in the Fashion MNIST test set, with dropout off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f42ddecd-9f36-4355-a3bc-38f642d71ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-8.10756256e-03, -2.28603570e-02, -4.82088628e-02,\n",
       "         -4.02934756e-02, -5.69705778e-02, -7.09606828e-02,\n",
       "         -9.84522372e-02, -1.55930716e-01, -2.38698115e-01,\n",
       "         -3.77802968e-01, -5.69786754e-01, -6.94190179e-01,\n",
       "         -7.58510471e-01, -7.49138359e-01, -7.48505062e-01,\n",
       "         -7.74760766e-01, -7.56736132e-01, -6.45892289e-01,\n",
       "         -5.03267070e-01, -3.19310624e-01, -2.02581239e-01,\n",
       "         -1.39231065e-01, -1.08753643e-01, -9.15151738e-02,\n",
       "         -6.79206056e-02, -5.08848835e-02, -3.37408758e-02,\n",
       "         -1.44791524e-02],\n",
       "        [-1.20517269e-02, -1.68631769e-02, -3.29494819e-02,\n",
       "         -5.55665050e-02, -7.76432672e-02, -1.31607643e-01,\n",
       "         -2.43396081e-01, -3.64492477e-01, -4.88346361e-01,\n",
       "         -6.43238232e-01, -8.32870874e-01, -1.00271146e+00,\n",
       "         -1.09306712e+00, -1.08386514e+00, -1.06741581e+00,\n",
       "         -1.10706092e+00, -1.08915445e+00, -9.38528726e-01,\n",
       "         -7.78471953e-01, -5.93507821e-01, -4.41483177e-01,\n",
       "         -3.16365678e-01, -2.07692820e-01, -1.48364190e-01,\n",
       "         -1.18267576e-01, -9.04785085e-02, -5.88949935e-02,\n",
       "         -2.95404587e-02],\n",
       "        [-1.52942664e-02, -2.54310596e-02, -4.54435412e-02,\n",
       "         -7.28474361e-02, -1.28987392e-01, -2.69280351e-01,\n",
       "         -4.04764455e-01, -5.03197851e-01, -6.30720006e-01,\n",
       "         -8.22157138e-01, -9.93965763e-01, -1.07218034e+00,\n",
       "         -1.10095463e+00, -1.09982387e+00, -1.09243762e+00,\n",
       "         -1.12132526e+00, -1.11728274e+00, -1.08121877e+00,\n",
       "         -9.76921890e-01, -7.76944292e-01, -5.93505822e-01,\n",
       "         -4.83432796e-01, -3.72197774e-01, -2.40341270e-01,\n",
       "         -1.69401286e-01, -1.29928465e-01, -8.80578669e-02,\n",
       "         -5.14606566e-02],\n",
       "        [-2.50385338e-02, -4.08662330e-02, -6.54114335e-02,\n",
       "         -1.02999979e-01, -2.14993315e-01, -3.71933206e-01,\n",
       "         -5.07100811e-01, -6.39677011e-01, -7.64494244e-01,\n",
       "         -9.34649192e-01, -1.05182744e+00, -1.09658275e+00,\n",
       "         -1.10539189e+00, -1.10544142e+00, -1.11313516e+00,\n",
       "         -1.13559987e+00, -1.15252051e+00, -1.14248513e+00,\n",
       "         -1.09120245e+00, -9.44343133e-01, -7.58788373e-01,\n",
       "         -6.18972478e-01, -4.87617122e-01, -3.49119799e-01,\n",
       "         -2.34162037e-01, -1.72464109e-01, -1.19987180e-01,\n",
       "         -7.31734917e-02],\n",
       "        [-3.84221462e-02, -6.21891887e-02, -9.05591689e-02,\n",
       "         -1.49584096e-01, -2.89608132e-01, -4.56067872e-01,\n",
       "         -6.11109685e-01, -7.21956828e-01, -8.21093516e-01,\n",
       "         -9.81713589e-01, -1.07969477e+00, -1.11191173e+00,\n",
       "         -1.11582610e+00, -1.12690934e+00, -1.15023764e+00,\n",
       "         -1.17576791e+00, -1.20365869e+00, -1.19941237e+00,\n",
       "         -1.16267194e+00, -1.03797712e+00, -8.58717327e-01,\n",
       "         -7.42229352e-01, -6.03877450e-01, -4.42694376e-01,\n",
       "         -3.10636464e-01, -2.28350074e-01, -1.64112098e-01,\n",
       "         -9.84337473e-02],\n",
       "        [-5.22635759e-02, -8.54040359e-02, -1.23061782e-01,\n",
       "         -2.00139209e-01, -3.50093065e-01, -5.33586003e-01,\n",
       "         -6.75633726e-01, -7.73004910e-01, -8.68288819e-01,\n",
       "         -1.02526294e+00, -1.10703831e+00, -1.13098047e+00,\n",
       "         -1.13219709e+00, -1.15840046e+00, -1.19755136e+00,\n",
       "         -1.22904386e+00, -1.27298539e+00, -1.28407562e+00,\n",
       "         -1.24619600e+00, -1.13428634e+00, -9.45072926e-01,\n",
       "         -8.32180768e-01, -7.05993808e-01, -5.33962354e-01,\n",
       "         -3.90380304e-01, -2.96454186e-01, -2.16233938e-01,\n",
       "         -1.25823858e-01],\n",
       "        [-6.74574128e-02, -1.08049013e-01, -1.54560969e-01,\n",
       "         -2.46153964e-01, -4.01544066e-01, -5.90099548e-01,\n",
       "         -7.21584976e-01, -8.14774839e-01, -8.98550354e-01,\n",
       "         -1.05180204e+00, -1.12866486e+00, -1.15469478e+00,\n",
       "         -1.15870217e+00, -1.19572978e+00, -1.24702430e+00,\n",
       "         -1.29928471e+00, -1.36479580e+00, -1.38378732e+00,\n",
       "         -1.33436009e+00, -1.21914498e+00, -1.02432365e+00,\n",
       "         -9.15230787e-01, -7.93676738e-01, -6.16836315e-01,\n",
       "         -4.61272337e-01, -3.63757806e-01, -2.61658154e-01,\n",
       "         -1.47848643e-01],\n",
       "        [-8.45664147e-02, -1.30368938e-01, -1.84957836e-01,\n",
       "         -2.85973864e-01, -4.43045571e-01, -6.32111498e-01,\n",
       "         -7.55277455e-01, -8.46577606e-01, -9.23792578e-01,\n",
       "         -1.06557325e+00, -1.14620884e+00, -1.17919683e+00,\n",
       "         -1.18809750e+00, -1.23482820e+00, -1.30170389e+00,\n",
       "         -1.38407038e+00, -1.47779894e+00, -1.50172227e+00,\n",
       "         -1.42807195e+00, -1.26505258e+00, -1.09467591e+00,\n",
       "         -9.96223641e-01, -8.72459010e-01, -6.09351636e-01,\n",
       "         -5.29166214e-01,  1.61176283e-01, -3.03890879e-01,\n",
       "         -1.64341818e-01],\n",
       "        [-9.60655201e-02, -1.49801023e-01, -2.11430849e-01,\n",
       "         -3.16445394e-01, -4.73202587e-01, -6.63228488e-01,\n",
       "         -7.76868522e-01, -8.67889934e-01, -9.40242613e-01,\n",
       "         -1.07130981e+00, -1.15911565e+00, -1.20415356e+00,\n",
       "         -1.22331564e+00, -1.27164933e+00, -1.32748374e+00,\n",
       "         -1.51572344e+00, -1.31599486e+00, -6.21773544e-01,\n",
       "         -1.38494619e+00, -1.36452956e+00, -1.18186722e+00,\n",
       "         -1.07346164e+00, -9.49830714e-01, -7.69468981e-01,\n",
       "         -5.96812242e-01,  1.21583263e+00, -3.47438868e-01,\n",
       "         -1.86271557e-01],\n",
       "        [-1.06383251e-01, -1.66656694e-01, -2.29073293e-01,\n",
       "         -3.36609633e-01, -4.94433332e-01, -6.83372449e-01,\n",
       "         -7.89910648e-01, -8.79757708e-01, -9.53325211e-01,\n",
       "         -1.07517277e+00, -1.17928632e+00, -1.23780932e+00,\n",
       "         -1.27290764e+00, -1.35370438e+00, -1.45128043e+00,\n",
       "         -1.74707534e+00, -7.43886122e-01,  1.95533038e-03,\n",
       "         -2.81491237e-01, -1.41020348e+00, -1.23866772e+00,\n",
       "         -1.12586282e+00, -1.00749004e+00, -5.85519680e-01,\n",
       "          4.65856263e-01,  8.40879235e-01, -3.97809423e-01,\n",
       "         -2.15043654e-01],\n",
       "        [-1.16331384e-01, -1.80071291e-01, -2.36956617e-01,\n",
       "         -3.41329729e-01, -5.04371497e-01, -6.88668657e-01,\n",
       "         -7.88764588e-01, -8.78326080e-01, -9.58716243e-01,\n",
       "         -1.07989456e+00, -1.22072422e+00, -1.30776777e+00,\n",
       "         -1.39775101e+00, -1.48575796e+00, -1.60520278e+00,\n",
       "         -1.26520542e+00, -2.91052661e-01, -3.99302967e-01,\n",
       "          9.28474696e-02,  5.66288625e-01,  4.01564161e-01,\n",
       "          5.91839278e-01,  3.87840496e-01,  8.19235303e-01,\n",
       "          1.26203650e+00,  1.10971813e+00, -4.54342668e-01,\n",
       "         -2.41094789e-01],\n",
       "        [-1.29339778e-01, -1.92211435e-01, -2.38326580e-01,\n",
       "         -3.35398055e-01, -4.99104931e-01, -6.79141724e-01,\n",
       "         -7.77511819e-01, -8.74237594e-01, -9.77174330e-01,\n",
       "         -1.12040239e+00, -1.33487525e+00, -1.47453111e+00,\n",
       "         -1.57767842e+00, -1.68447172e+00, -1.55135465e+00,\n",
       "         -1.98380575e-01, -3.17347685e-01, -3.44111099e-01,\n",
       "          1.90045333e-01,  5.27293192e-01,  3.79542812e-01,\n",
       "          5.68388504e-01,  8.03834426e-01,  7.00543801e-01,\n",
       "          9.94113022e-01,  1.04888560e+00, -5.06190530e-01,\n",
       "         -2.69621135e-01],\n",
       "        [-1.43476822e-01, -2.03872032e-01, -2.45757662e-01,\n",
       "         -3.38478811e-01, -5.04480834e-01, -6.74556909e-01,\n",
       "         -7.66684540e-01, -8.89554824e-01, -1.01607848e+00,\n",
       "         -1.23461241e+00, -1.51599756e+00, -1.60432009e+00,\n",
       "         -1.75349877e+00, -1.77529852e+00, -2.75408235e-01,\n",
       "         -5.40745951e-01, -6.90702186e-01, -2.51772331e-01,\n",
       "          2.42124548e-01,  2.07403070e-01,  2.95941427e-01,\n",
       "          4.98310414e-01,  6.34529944e-01,  5.73393833e-01,\n",
       "          1.00929543e+00,  1.15772197e+00, -3.99247956e-01,\n",
       "         -3.03138189e-01],\n",
       "        [-1.52358111e-01, -2.19310117e-01, -2.63852809e-01,\n",
       "         -3.59922044e-01, -5.23573771e-01, -7.13108474e-01,\n",
       "         -8.28623021e-01, -9.53143620e-01, -1.12403193e+00,\n",
       "         -1.35259858e+00, -1.58571988e+00, -1.76342932e+00,\n",
       "         -1.88604204e+00, -6.87009925e-01,  6.73458052e-03,\n",
       "         -8.82447410e-01, -8.84713624e-01, -3.73732780e-02,\n",
       "         -3.88358147e-02, -6.01405562e-02,  2.42356319e-01,\n",
       "          5.18013266e-01,  6.55662716e-01,  5.38545284e-01,\n",
       "          1.00279714e+00,  1.26387825e+00,  2.33218964e-02,\n",
       "         -3.34298949e-01],\n",
       "        [-1.82237724e-01, -2.72651371e-01, -3.25050866e-01,\n",
       "         -4.25643877e-01, -6.15719076e-01, -7.96249270e-01,\n",
       "         -8.77939371e-01, -9.59791865e-01, -1.15606812e+00,\n",
       "         -1.43397296e+00, -1.73773310e+00, -1.91968799e+00,\n",
       "         -6.91117419e-01, -9.82911340e-02, -3.69567972e-01,\n",
       "         -6.17048907e-01, -6.77824826e-01,  7.59647261e-02,\n",
       "         -2.57586896e-01,  1.01856264e-01,  2.50635596e-01,\n",
       "          4.17997480e-01,  6.01614190e-01,  5.35516013e-01,\n",
       "          9.77569677e-01,  1.25489212e+00,  8.23613012e-01,\n",
       "         -3.63546504e-01],\n",
       "        [-2.49525785e-01, -3.53273569e-01, -3.62329857e-01,\n",
       "         -4.64743525e-01, -6.77121442e-01, -8.34503966e-01,\n",
       "         -9.43620197e-01, -1.03719354e+00, -1.22280915e+00,\n",
       "         -1.55252779e+00, -1.56339866e+00, -5.97212218e-01,\n",
       "         -4.96052621e-01, -6.14381893e-01, -3.73971090e-01,\n",
       "         -5.18316505e-01, -3.37840827e-01, -3.95007676e-02,\n",
       "         -3.12011591e-01,  2.05615083e-01,  3.18929265e-01,\n",
       "          3.80445224e-01,  5.17421961e-01,  4.85446901e-01,\n",
       "          8.22534408e-01,  1.08195260e+00,  1.44337070e+00,\n",
       "         -3.90078103e-01],\n",
       "        [-2.08247474e-01, -4.02804889e-01, -4.45937322e-01,\n",
       "         -5.47800752e-01, -7.47617107e-01, -9.04929634e-01,\n",
       "         -9.92960647e-01, -8.60031673e-01, -6.87917682e-01,\n",
       "         -5.47647493e-01, -4.64045089e-01, -6.40961880e-01,\n",
       "         -7.43127100e-01, -4.31588628e-01, -1.74349728e-01,\n",
       "         -3.01497627e-01, -2.77932226e-01, -9.75364110e-02,\n",
       "          1.08315575e-01,  3.91094294e-01,  3.45025128e-01,\n",
       "          3.41845597e-01,  4.60304617e-01,  4.99964026e-01,\n",
       "          7.07709763e-01,  1.01615267e+00,  1.28388636e+00,\n",
       "          7.41150971e-01],\n",
       "        [-3.21566417e-01, -4.32156916e-01, -1.22178672e-01,\n",
       "          1.60191008e-01, -1.05268297e-02, -1.05294953e-01,\n",
       "         -1.18501888e-01,  1.74495293e-01,  1.36510487e-01,\n",
       "         -2.18543288e-01, -5.99431336e-01, -6.49116349e-01,\n",
       "         -5.52914839e-01, -2.71232974e-01, -2.27021488e-01,\n",
       "         -2.17118720e-01, -4.20924524e-01, -3.45014559e-01,\n",
       "          8.47445785e-02,  6.76704007e-02,  4.30478278e-01,\n",
       "          5.05299998e-01,  3.99942472e-01,  6.65932793e-01,\n",
       "          1.13280596e+00,  1.20923458e+00,  1.65224598e+00,\n",
       "          1.34628765e+00],\n",
       "        [-2.93350605e-01,  7.49574961e-01,  9.67133778e-01,\n",
       "          6.34125281e-01,  5.23578292e-01,  3.08387139e-01,\n",
       "          1.55186705e-01,  1.39162374e-01, -2.45529149e-02,\n",
       "         -1.63513781e-01, -2.38216793e-01, -2.78579228e-01,\n",
       "         -1.57210807e-01, -2.32466029e-01, -1.56265965e-01,\n",
       "          1.11251583e-02, -4.50786694e-02, -1.62018489e-01,\n",
       "          7.88195869e-04, -4.68229273e-02,  5.81295438e-01,\n",
       "          5.77335170e-01,  4.43383339e-01,  5.21062937e-01,\n",
       "          9.64841246e-01,  1.29091143e+00,  1.84193179e+00,\n",
       "          1.33313473e+00],\n",
       "        [ 2.51711354e+00,  2.57864197e+00,  1.54572413e+00,\n",
       "          8.36900186e-01,  3.74955704e-01,  1.73476620e-01,\n",
       "         -3.63743801e-03, -1.88863849e-02, -2.03691634e-01,\n",
       "         -4.26729954e-01, -5.45823892e-01, -6.30939556e-01,\n",
       "         -4.34434627e-01, -2.65618348e-01, -9.14662923e-02,\n",
       "          3.47156643e-02,  3.36565909e-02,  2.22896357e-01,\n",
       "          5.97597668e-01,  7.60282473e-01,  1.02977122e+00,\n",
       "          1.02109553e+00,  1.05604935e+00,  1.10507117e+00,\n",
       "          1.54130007e+00,  1.70723468e+00,  1.85260500e+00,\n",
       "          8.42101175e-01],\n",
       "        [ 3.70244723e-01,  1.85782611e+00,  2.25309895e+00,\n",
       "          2.02959980e+00,  1.48041548e+00,  1.12123509e+00,\n",
       "          8.51476696e-01,  6.54359228e-01,  2.51471010e-01,\n",
       "         -1.49327457e-01, -2.32607138e-01, -2.70843962e-01,\n",
       "         -1.30546916e-02,  5.83301962e-01,  7.46525024e-01,\n",
       "          8.44514742e-01,  7.71810187e-01,  1.38813376e+00,\n",
       "          8.98026533e-01,  6.80561728e-01,  1.66610851e+00,\n",
       "          1.69388612e+00,  1.68437514e+00,  1.89645599e+00,\n",
       "          2.26790867e+00,  2.28083706e+00,  2.43473433e+00,\n",
       "          1.50276804e+00],\n",
       "        [-2.46719269e-01, -3.52721015e-01, -4.21804277e-01,\n",
       "         -3.70952814e-01,  7.07974930e-02,  3.08242041e-01,\n",
       "          8.22425103e-01,  1.07126281e+00,  9.95192882e-01,\n",
       "          9.35989016e-01,  8.67773930e-01,  8.06952542e-01,\n",
       "          7.92343091e-01,  6.76509997e-01,  2.81590961e-01,\n",
       "         -5.76295233e-01, -1.50426267e+00, -1.52647078e+00,\n",
       "         -1.38007400e+00, -1.17319388e+00,  1.00499312e+00,\n",
       "          1.32159845e+00,  1.09295458e+00,  1.13532120e+00,\n",
       "          1.34367792e+00,  1.63014007e+00,  1.86050390e+00,\n",
       "          1.60311783e-01],\n",
       "        [-2.18762400e-01, -3.11349656e-01, -3.79537820e-01,\n",
       "         -4.98254811e-01, -6.86431029e-01, -7.87174583e-01,\n",
       "         -8.51289164e-01, -9.15042226e-01, -1.11390616e+00,\n",
       "         -1.31818590e+00, -1.50796759e+00, -1.56632633e+00,\n",
       "         -1.53590647e+00, -1.37945427e+00, -1.27545598e+00,\n",
       "         -1.35840535e+00, -1.41271931e+00, -1.35288316e+00,\n",
       "         -1.23994843e+00, -1.07520670e+00, -9.03019653e-01,\n",
       "         -8.36605577e-01, -8.11486638e-01, -7.55604629e-01,\n",
       "         -6.15431703e-01, -4.66520091e-01, -3.70713336e-01,\n",
       "         -2.29578593e-01],\n",
       "        [-1.85117565e-01, -2.63459224e-01, -3.33542847e-01,\n",
       "         -4.53997423e-01, -6.35221516e-01, -7.27137432e-01,\n",
       "         -7.83534585e-01, -8.33864131e-01, -1.01643747e+00,\n",
       "         -1.19021138e+00, -1.34880074e+00, -1.40883131e+00,\n",
       "         -1.38171374e+00, -1.24806432e+00, -1.15342590e+00,\n",
       "         -1.21296423e+00, -1.26263347e+00, -1.22397570e+00,\n",
       "         -1.12759311e+00, -9.86650483e-01, -8.28917745e-01,\n",
       "         -7.64122334e-01, -7.44643484e-01, -6.91878142e-01,\n",
       "         -5.57511912e-01, -4.07885593e-01, -3.13113355e-01,\n",
       "         -1.87556207e-01],\n",
       "        [-1.39772956e-01, -2.11082295e-01, -2.83363323e-01,\n",
       "         -4.06856788e-01, -5.85023636e-01, -6.73459363e-01,\n",
       "         -7.24290700e-01, -7.59782004e-01, -9.19506114e-01,\n",
       "         -1.06886305e+00, -1.20421594e+00, -1.26466412e+00,\n",
       "         -1.24222542e+00, -1.12857217e+00, -1.03850839e+00,\n",
       "         -1.08971331e+00, -1.13961280e+00, -1.11672256e+00,\n",
       "         -1.03324894e+00, -9.04406557e-01, -7.59012975e-01,\n",
       "         -7.03741360e-01, -6.90560172e-01, -6.38373181e-01,\n",
       "         -5.06378061e-01, -3.54924201e-01, -2.61559117e-01,\n",
       "         -1.50395802e-01],\n",
       "        [-1.00080238e-01, -1.61440183e-01, -2.33652743e-01,\n",
       "         -3.58612695e-01, -5.33705269e-01, -6.20613370e-01,\n",
       "         -6.64108545e-01, -6.79240156e-01, -8.09563234e-01,\n",
       "         -9.40545891e-01, -1.05342306e+00, -1.11277053e+00,\n",
       "         -1.09818492e+00, -1.00427799e+00, -9.27337059e-01,\n",
       "         -9.72267620e-01, -1.02506621e+00, -1.01105415e+00,\n",
       "         -9.35794330e-01, -8.14713645e-01, -6.82477741e-01,\n",
       "         -6.45033241e-01, -6.37538376e-01, -5.82491150e-01,\n",
       "         -4.52323894e-01, -2.97301075e-01, -2.08848530e-01,\n",
       "         -1.10789442e-01],\n",
       "        [-5.60055659e-02, -1.04085247e-01, -1.77207615e-01,\n",
       "         -3.05389703e-01, -4.75292621e-01, -5.59726405e-01,\n",
       "         -5.95074722e-01, -5.88956822e-01, -6.79526042e-01,\n",
       "         -8.09933602e-01, -9.17561280e-01, -9.73258093e-01,\n",
       "         -9.59636772e-01, -8.80732079e-01, -8.15637086e-01,\n",
       "         -8.60612610e-01, -9.15732863e-01, -9.06892070e-01,\n",
       "         -8.31754761e-01, -7.02304657e-01, -5.82791817e-01,\n",
       "         -5.73196129e-01, -5.74695184e-01, -5.20074404e-01,\n",
       "         -3.97156170e-01, -2.41654513e-01, -1.54498161e-01,\n",
       "         -7.50588115e-02],\n",
       "        [-2.45250161e-02, -4.82037608e-02, -1.01600908e-01,\n",
       "         -2.09072195e-01, -3.40741661e-01, -4.26791201e-01,\n",
       "         -4.37007660e-01, -4.05842095e-01, -4.57010834e-01,\n",
       "         -5.54491702e-01, -6.71804037e-01, -7.51177643e-01,\n",
       "         -7.52891583e-01, -6.89201810e-01, -6.44103254e-01,\n",
       "         -6.77292054e-01, -7.28845710e-01, -7.02128562e-01,\n",
       "         -5.99645356e-01, -4.74555130e-01, -3.94940372e-01,\n",
       "         -4.07819392e-01, -4.42067930e-01, -3.96186313e-01,\n",
       "         -2.87358942e-01, -1.56166422e-01, -8.90840596e-02,\n",
       "         -3.39927363e-02]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a9f67707-113a-4375-b2d1-6692b970454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   ,\n",
       "        0.999]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6e105-1ba2-4dac-ac66-be318c63c4ed",
   "metadata": {},
   "source": [
    "The model seems almost certain that this image belongs to class 9 (ankle boot). Should you trust it? Is there really so little room for doubt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054c2c4-2b52-4fec-b6e1-a7bc19ef5515",
   "metadata": {},
   "source": [
    "Compare this with the predictions made when dropout is activated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8cabc328-354e-4abc-8f9b-bd390d173f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.   , 0.   , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.006, 0.   ,\n",
       "         0.983]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.029, 0.   , 0.186, 0.   ,\n",
       "         0.785]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.161, 0.   , 0.231, 0.   ,\n",
       "         0.607]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.036, 0.   , 0.029, 0.   ,\n",
       "         0.934]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.068, 0.   , 0.084, 0.   ,\n",
       "         0.848]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.037, 0.   , 0.296, 0.   ,\n",
       "         0.667]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.048, 0.   , 0.159, 0.   ,\n",
       "         0.793]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.392, 0.   ,\n",
       "         0.607]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.005, 0.   , 0.035, 0.   ,\n",
       "         0.96 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.57 , 0.   ,\n",
       "         0.428]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.036, 0.   , 0.092, 0.   ,\n",
       "         0.872]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.041, 0.   , 0.045, 0.   ,\n",
       "         0.914]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.056, 0.   , 0.114, 0.   ,\n",
       "         0.829]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.021, 0.   ,\n",
       "         0.978]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.003, 0.   , 0.129, 0.   ,\n",
       "         0.868]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.058, 0.   ,\n",
       "         0.941]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.009, 0.   , 0.06 , 0.   ,\n",
       "         0.93 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.005, 0.   ,\n",
       "         0.994]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.007, 0.   , 0.114, 0.   ,\n",
       "         0.879]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.003, 0.   , 0.067, 0.   ,\n",
       "         0.929]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.094, 0.   , 0.024, 0.003,\n",
       "         0.878]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.028, 0.   , 0.115, 0.   ,\n",
       "         0.857]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.009, 0.   ,\n",
       "         0.991]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.006, 0.   , 0.451, 0.   ,\n",
       "         0.542]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.009, 0.   , 0.016, 0.   ,\n",
       "         0.974]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.013, 0.   , 0.136, 0.   ,\n",
       "         0.851]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.003, 0.   ,\n",
       "         0.997]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.044, 0.   , 0.07 , 0.   ,\n",
       "         0.886]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.02 , 0.   , 0.015, 0.   ,\n",
       "         0.965]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.015, 0.   , 0.16 , 0.   ,\n",
       "         0.824]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.019, 0.   ,\n",
       "         0.97 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.112, 0.   , 0.026, 0.   ,\n",
       "         0.861]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.066, 0.   , 0.243, 0.   ,\n",
       "         0.691]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.007, 0.   , 0.015, 0.   ,\n",
       "         0.978]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.029, 0.   , 0.049, 0.   ,\n",
       "         0.922]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.052, 0.   , 0.085, 0.   ,\n",
       "         0.863]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.06 , 0.   , 0.317, 0.   ,\n",
       "         0.623]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.026, 0.   ,\n",
       "         0.972]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.004, 0.   , 0.001, 0.   ,\n",
       "         0.995]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.013, 0.   ,\n",
       "         0.986]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.039, 0.   ,\n",
       "         0.959]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.004, 0.   , 0.03 , 0.   ,\n",
       "         0.966]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.046, 0.   , 0.183, 0.   ,\n",
       "         0.771]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.014, 0.   ,\n",
       "         0.986]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.003, 0.   , 0.133, 0.   ,\n",
       "         0.863]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.006, 0.   , 0.17 , 0.   ,\n",
       "         0.824]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.008, 0.   , 0.052, 0.   ,\n",
       "         0.94 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.019, 0.   , 0.043, 0.   ,\n",
       "         0.938]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.224, 0.   ,\n",
       "         0.774]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.05 , 0.   ,\n",
       "         0.948]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.013, 0.   ,\n",
       "         0.985]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.027, 0.   ,\n",
       "         0.962]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.003, 0.   , 0.229, 0.   ,\n",
       "         0.768]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.071, 0.   , 0.581, 0.   ,\n",
       "         0.347]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.019, 0.   ,\n",
       "         0.981]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.008, 0.   , 0.04 , 0.   ,\n",
       "         0.952]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.047, 0.   , 0.008, 0.   ,\n",
       "         0.945]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.003, 0.   , 0.169, 0.   ,\n",
       "         0.828]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.1  , 0.   , 0.369, 0.001,\n",
       "         0.53 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.051, 0.   , 0.383, 0.   ,\n",
       "         0.566]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.026, 0.   ,\n",
       "         0.973]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.067, 0.   ,\n",
       "         0.932]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.06 , 0.   ,\n",
       "         0.938]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.023, 0.   ,\n",
       "         0.976]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.028, 0.   ,\n",
       "         0.97 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.072, 0.   , 0.098, 0.   ,\n",
       "         0.83 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.012, 0.   , 0.02 , 0.   ,\n",
       "         0.968]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.007, 0.   , 0.053, 0.   ,\n",
       "         0.94 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.002, 0.   ,\n",
       "         0.997]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.003, 0.   , 0.105, 0.   ,\n",
       "         0.892]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.273, 0.   , 0.12 , 0.   ,\n",
       "         0.607]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.009, 0.   , 0.057, 0.   ,\n",
       "         0.934]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.018, 0.   , 0.155, 0.   ,\n",
       "         0.827]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.014, 0.   , 0.018, 0.   ,\n",
       "         0.968]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.002, 0.   ,\n",
       "         0.996]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.02 , 0.   , 0.021, 0.   ,\n",
       "         0.959]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.006, 0.   , 0.015, 0.   ,\n",
       "         0.979]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.012, 0.   , 0.134, 0.   ,\n",
       "         0.855]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.298, 0.   , 0.178, 0.   ,\n",
       "         0.524]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.005, 0.   , 0.106, 0.   ,\n",
       "         0.889]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.005, 0.   , 0.136, 0.   ,\n",
       "         0.859]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.011, 0.   ,\n",
       "         0.987]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.093, 0.   , 0.229, 0.   ,\n",
       "         0.677]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.003, 0.   ,\n",
       "         0.996]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.069, 0.   , 0.124, 0.   ,\n",
       "         0.807]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.051, 0.   ,\n",
       "         0.948]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.054, 0.   ,\n",
       "         0.945]],\n",
       "\n",
       "       [[0.   , 0.001, 0.001, 0.005, 0.001, 0.07 , 0.024, 0.28 , 0.   ,\n",
       "         0.617]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.049, 0.   ,\n",
       "         0.949]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   , 0.017, 0.   ,\n",
       "         0.98 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.019, 0.   ,\n",
       "         0.981]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.024, 0.   , 0.27 , 0.   ,\n",
       "         0.707]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.031, 0.   , 0.139, 0.   ,\n",
       "         0.83 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.006, 0.   , 0.114, 0.   ,\n",
       "         0.88 ]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.068, 0.   ,\n",
       "         0.922]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.024, 0.   , 0.207, 0.   ,\n",
       "         0.769]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.117, 0.   , 0.424, 0.   ,\n",
       "         0.459]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.044, 0.   ,\n",
       "         0.956]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.064, 0.   , 0.011, 0.   ,\n",
       "         0.926]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.062, 0.   ,\n",
       "         0.937]]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eaf8da-3081-42b2-bef8-47e5935212fb",
   "metadata": {},
   "source": [
    "This tells a very different story: as you can see that there are some cases where apparently, when we activate dropout, the model is not sure anymore. It still seems to prefer class 9, but sometimes it hesitates with classes 5 (sandal) and 7 (sneaker), which makes sense given they’re all footwear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1793c-9da9-413b-9827-e507ae2c3f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16582747-2c77-4e45-b1a1-e7ba9bc3c0ae",
   "metadata": {},
   "source": [
    "Once we average over the first dimension, we get the following MC Dropout predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebbcea25-111c-4212-ab4e-99fd8499a95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.11, 0.  , 0.86]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678cfc8-127c-46b0-bd2b-b78013da01c0",
   "metadata": {},
   "source": [
    "The model still thinks this image belongs to class 9, but only with a 86% confidence, which seems much more reasonable than 99%. Plus it’s useful to know exactly which other classes it thinks are likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cdee93-814a-4d10-b11c-1a15f47a7d35",
   "metadata": {},
   "source": [
    "And you can also take a look at the standard deviation of the probability estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f95ba739-231b-44c2-9885-b840efd14ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a9aaa6c-a534-4df4-b4aa-c8f83bef220a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.12, 0.  , 0.14]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94dba1a7-42a1-4ab9-876a-22abe6865b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula to calculate accuracy on your own\n",
    "# accuracy = np.sum(y_pred == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c80f0ab-54e9-469d-9359-3c046ff5ee08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c6a695b-d2bb-4c29-bb0a-979593f017cc",
   "metadata": {},
   "source": [
    "## Training with different model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235a8ca-ddd2-4f75-beb9-a26eb4436560",
   "metadata": {},
   "source": [
    "If your model contains other layers that behave in a special way during training (such as BatchNormalization layers), then you should not force training mode like we just did. Instead, you should replace the Dropout layers with the following MCDropout class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e4f07d3-4567-41d3-b847-f39e38e3b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b41a2-8f20-4b13-915d-ee1d8fd7b02d",
   "metadata": {},
   "source": [
    "Here, we just subclass the Dropout layer and override the call() method to force its training argument to True. Similarly, you could define an MCAlpha Dropout class by subclassing AlphaDropout instead. If you are creating a model from scratch, it’s just a matter of using MCDropout rather than Dropout. But if you have a model that was already trained using Dropout, then in that case you need to create a new model that’s identical to the existing model except that it replaces the Dropout layers with MCDropout, then copy the existing model’s weights to your new model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744dca2-95a4-4706-a607-5b2ae061af78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b91589d-958f-4b6d-9835-b63dc067e2d8",
   "metadata": {},
   "source": [
    "## Max-Norm Regularization\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8fbeb-e8f9-431e-a78d-1722a1ac9569",
   "metadata": {},
   "source": [
    "Another regularization technique that is popular for neural networks is called maxnorm regularization: for each neuron, it constrains the weights w of the incoming connections such that ∥ w ∥2 ≤ r, where r is the max-norm hyperparameter and ∥ · ∥2 is the ℓ2 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e5ed9-4940-4d78-9495-ace2abb05781",
   "metadata": {},
   "source": [
    "Max-norm regularization does not add a regularization loss term to the overall loss function. Instead, it is typically implemented by computing ∥w∥2 after each training step and rescaling w if needed (w ← w r/‖ w ‖2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55ca66-3780-423b-baa9-aab2c9d05cf2",
   "metadata": {},
   "source": [
    "Reducing r increases the amount of regularization and helps reduce overfitting. Maxnorm regularization can also help alleviate the unstable gradients problems (if you are not using Batch Normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2996025-3c26-4ccc-b3c4-b436b68bc883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baeb0147-910b-4f5e-a7c1-a130baea9bf5",
   "metadata": {},
   "source": [
    "#### Implementing Max Norm in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9188cf3-da59-4f94-9b9d-1d33bfe87c2e",
   "metadata": {},
   "source": [
    "To implement max-norm regularization in Keras, set the kernel_constraint argument of each hidden layer to a max_norm() constraint with the appropriate max value, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0317edb8-6ac8-487e-ae71-cbc711531d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x2078fd80b50>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbd508-3d80-4982-ad98-5d77e726193f",
   "metadata": {},
   "source": [
    "After each training iteration, the model’s fit() method will call the object returned by max_norm(), passing it the layer’s weights and getting rescaled weights in return, which then replace the layer’s weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548c47f-4154-4567-b6e0-bc2d9312bbd8",
   "metadata": {},
   "source": [
    "##### Max Norm usage Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7c919-0c11-4058-8bd0-e7bb12a6a55e",
   "metadata": {},
   "source": [
    "The max_norm() function has an axis argument that defaults to 0. A Dense layer usually has weights of shape [number of inputs, number of neurons], so using axis=0 means that the max-norm constraint will apply independently to each neuron’s weight vector. If you want to use max-norm with convolutional layers, make sure to set the max_norm() constraint’s axis argument appropriately (usually axis=[0, 1, 2])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca868851-135a-4505-b7f6-8420435459ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb5ff96f-32ae-4a68-9990-026e57842d49",
   "metadata": {},
   "source": [
    "## Summary and Practical Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f904b4-6788-4b7c-801d-e929e1618b23",
   "metadata": {},
   "source": [
    "In this section we'll show some general configuration that are suitable for many cases, HOWEVER, UNDER NO CIRCUMSTANCES SHOULD THEY BE CONSIDERED AS DEFAULT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753be045-d727-4d88-a76a-b43cfeb3ba98",
   "metadata": {},
   "source": [
    "| Hyperparameter | Default value |\n",
    "| --- | --- |\n",
    "|Kernel initializer|He initialization|\n",
    "|Activation function|ELU|\n",
    "|Normalization|None if shallow; Batch Norm if deep|\n",
    "|Regularization|Early stopping (+ℓ2 reg. if needed)|\n",
    "|Optimizer|Momentum optimization (or RMSProp or Nadam)|\n",
    "|Learning rate|schedule 1cycle|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe88b2-cd5e-4d70-aab2-93c33c7eee00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f821d6b3-21b7-4df2-bbe4-fa59420da9df",
   "metadata": {},
   "source": [
    "If the network is a simple stack of dense layers, then it can self-normalize, and you should use the configuration in Table shown below instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616cc9b2-07de-4353-bcf9-fee03ef783c2",
   "metadata": {},
   "source": [
    "| Hyperparameter | Default value |\n",
    "| --- | --- |\n",
    "|Kernel initializer|LeCun initialization|\n",
    "|Activation function|SELU|\n",
    "|Normalization|None (self-normalization)|\n",
    "|Regularization|Alpha dropout if needed|\n",
    "|Optimizer|Momentum optimization (or RMSProp or Nadam)|\n",
    "|Learning rate|schedule 1cycle|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d4d1e-289b-45f0-b1cb-a0b538225eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d81782ed-8466-4fab-8b90-d6724aefe594",
   "metadata": {},
   "source": [
    "### Guidelines!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e89435-3bde-4d1a-9881-bf8849f9730e",
   "metadata": {},
   "source": [
    "#### 1) Don’t forget to normalize the input features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63e0b6-b706-4ead-9ef7-648959a9130d",
   "metadata": {},
   "source": [
    "#### 2) You should also try to reuse parts of a pretrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of unlabeled data, or use pretraining on an auxiliary task if you have a lot of labeled data for a similar task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46d739-d797-48c3-b38e-b7b7de78094b",
   "metadata": {},
   "source": [
    "#### 3)If you need a sparse model, you can use ℓ1 regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can use the TensorFlow Model Optimization Toolkit. This will break self-normalization, so you should use the default configuration in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9dab56-d0fb-4bd2-a545-6faa7510ef6b",
   "metadata": {},
   "source": [
    "#### 4) If you need a low-latency model (one that performs lightning-fast predictions), you may need to use fewer layers, fold the Batch Normalization layers into the previous layers, and possibly use a faster activation function such as leaky ReLU or just ReLU. Having a sparse model will also help. Finally, you may want to reduce the float precision from 32 bits to 16 or even 8 bits (see “Deploying a Model to a Mobile or Embedded Device” on page 685). Again, check out TFMOT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5136911-c735-436e-aaac-fb32e2c05f9c",
   "metadata": {},
   "source": [
    "#### 5) If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c7d4d-7637-4e69-aa94-2815135b017a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd0ca7-348a-429d-b6a2-39c3a43728a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b77fd-5bed-41cd-9235-fd47443f8b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77faed8-9716-48de-aa17-1ba8d3bf5f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5674a2ce-8dfe-4898-aca0-182d12e1e507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb723a-0059-4b0b-9786-28bba61fa04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b8748-f618-404c-9b91-4d2a3e9ffdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3868d8a0-54b6-48ef-ab08-c13d3c17ba44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7986d-ced7-496a-9b77-76d6e3d6018b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba8294-6325-4df8-8ca3-21a90ae47850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d9ffa-9058-4221-a9bf-4355f45e211b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7665539b-1b04-436e-8a64-fd6ba5569363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f32529-74a3-42db-924a-15326678d547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991b837-3908-4b1f-88f1-b7fe95584da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82e740-3829-452a-9ffc-32de04a8fc50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4909780-dd98-445e-935c-f8a6a002c595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c2a60-50dd-4f9d-8ffb-4a6eddcb4758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579967d3-62b1-4479-8822-ae64ed09b68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029edd6-738d-448f-97eb-93ee199e2d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1b6ac-9ce3-4f06-b8fe-052117e492e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759553cb-8f6f-43a8-9a1a-bacc8f4754b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad51ee7-c5dd-40a5-ae53-8710df6919a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf873a8-8778-4355-80ba-c772837a34d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63e27a-fdb8-4348-8b9c-bda6cffec660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be08f3-8e61-4547-bce8-88b652787d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a095d-aa9e-41b9-a66d-35565b610116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20a5d4-b0d9-4ec3-951f-755b8ec1af50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e2874-170b-4479-a43b-0a005877f6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedcec2e-2ccf-421e-b8da-d8ad70d70913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1fb34-ebf8-4f50-8765-ed1473d4cc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c42220-be5f-4bf6-9ccd-e44b4f3d7d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486898a4-155a-4970-8499-6bab3f134306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeabd75-e9cc-4f22-9ecf-ca5f8a852ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d81d2b-1fe8-434b-9bd6-4169236bf5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51809f-8a05-4148-8973-5328acf1697c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984ccd7-95fe-450f-a34d-7ef88ab49021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfaf243-399e-4c80-a39f-d46df71104e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204889ab-116a-4e96-a8b2-37f734cc0abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e20405-34fd-4ec9-a0f6-c59f44038b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f6970-fd5d-4d13-af66-833174049378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194784c-d88d-4faf-932b-65a423f80b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a70baa6-0b99-44b4-936d-a1ecf5f6b6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5ba01-1ac8-4f0f-aa81-9347a0fdac12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ef9b9-aa22-4ca0-9258-f64762d7de8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301aec1-46a3-4d7a-b3a9-852e0f696178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a45faf-09d6-471e-b96e-8ad5e83607f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8471a5b-3a07-48a5-98ce-e05cbffe1d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f2513-6a19-4523-bf65-dda86902c03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66525cdb-5b30-4bb8-945e-e6988ef0f63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84ad45-242a-4706-b0c5-7f5ad40426b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7210f7f-e754-423b-8684-ded7eff1ed21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edf879-9c68-44c1-b4aa-129b5c52fb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c73efd-385e-444d-a570-97b54de8836e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414984f-ef1b-4385-ac05-f8324ee5e5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593930e0-c606-4caa-9655-d90caeb39aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b521bc-4bc3-4d4e-81fa-fc149b983676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189cb5bf-9995-4963-bf77-221b138dbb12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269fe86-06a0-4dd6-93c9-f029d17a8276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc21bfc-5514-42cb-aec7-c0985918ae8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2a173-18d3-42a5-9416-293175a30814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fd682-cef5-4724-8cd6-faac80ffe91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac6959-aec0-40b6-af30-8fe69dcbb659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a8479-5017-43e6-9ce7-7396f98f2f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b0e4b-32d6-454e-bb60-49a96c66baa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb46b9f-b9cc-468f-a341-b46ab03e9680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1602bf-3ba6-4235-ad58-dfa9edc5d00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428f38f-2873-4752-a624-7fab8097f814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36593cf5-c29c-4a64-9598-56972b25bfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb0d56-8021-4de8-a640-e6e92379e09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf4e6d7-3183-4103-93f4-41403ea4f444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a84439-250e-4c88-a399-2617ded163bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a89fa0-aaba-4daf-8d5e-fb65b07d93b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e08ea-d261-4b05-a8d8-420073179831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c489844-e2a6-46a1-8a53-3c36878b3ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549c1c8-8c31-4e54-a969-46631976e1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd9a7c-29e9-440c-a668-3c2be17e339a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22c4d3-a3e8-4dfa-b928-33202b848600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8342dda-2862-4efc-892a-caf89c0e139a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614958d-58ff-4abc-a265-2f3da08985db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9df8b-21ba-4532-b47f-2ce19ceaf051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb4d7a-7e07-4409-a63e-34a9b245a352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4becffa-2d23-4d41-90fc-ae9f3724f71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d312e-208f-49bb-8975-d0db16cfc06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d506e-d8a0-46c9-a2be-f3ccf51e85f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55951da5-5acc-4c44-8453-fb2ccf6794e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850d366-5ef9-4069-a466-ab45419cbd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de65ff9-63cf-45b9-9119-58fba809cf72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17796a33-4e63-4a50-94e2-69f0bd98c316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521521c6-8f65-4ed8-a512-0f01ca8edff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a04a60-41f3-44ca-aead-8193c6cec471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e5023-b7d6-4eed-b02f-681878888e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5a21f-a9bc-4ed5-a123-80d73fac63aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d520a-4d5c-450b-aa02-d9db7dc7074c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8680172-711a-4fc6-a17e-997d77738437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff4eec-c6e2-432e-b00f-6977e24a578d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ee48d-9cf3-48f2-8af2-3983acf6119e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d690510f-24f3-4999-aef1-d0c941b2e9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f5532-05d1-4172-a01e-b28a85aea7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1bc352-5624-40dd-b274-1fe7ddee7233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04459a-8b1b-4582-9b40-ba9dbe5df739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2548bd-7fe4-4752-9ca8-130de1e1e61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d1405-d7b5-46d0-8583-6a1b9473d550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c042d9f-985a-4f6e-9423-0b4e9e9abf33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b1760-ee95-4961-92ae-a375b46600a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149c8e3-9557-4b3b-9c30-0eddcf39f474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b88e1-1bc1-4015-8644-24fe6e4f8ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5ea9e-9c94-4e73-a15e-4edf4cbea28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fd483-aa35-41f0-a18b-4d6afc3fe876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a9f2c-e8e5-41d9-8d92-29ad0cccbeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b1469-cf00-42e7-adff-bd20500da42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f299bd-a7c8-4158-865c-8d25196f60f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a572b-e114-4d01-b975-ae7bffce6175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502606c-6a5f-4ec8-b2bf-90b8fefeaf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9f5a1-bdee-4daf-9d54-adca7e5eea2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69a7d6-091b-4b69-b4e5-ebf3deaafba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9f666-3f42-4dc7-b519-0ec40b26c193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d16891-b32f-4e8f-a9ed-efa473b009f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c937dc-6dcd-494d-83d1-a77041ab5d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a602cbdf-66b6-43b8-bd32-17afed659074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b19f58-9337-49bc-83bf-78b77456b6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d2816-821e-4881-88cb-88eef7612e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac45812-615d-4a6e-a0fc-bf50e4f4d822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822a673-dbf9-4875-b149-1a7b9a591c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d7f36-30d3-4bd7-bf30-5ca4a748c4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0778f4-b6c9-4a5b-945a-4d820a63ce28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94830177-265b-452c-8156-64224468e4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d997ac-1728-48c4-8667-71979c969936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61b7c8-5dd3-45c1-8786-e765df8be8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e116c-0822-415e-8562-2d29b423d121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aedf4b-5724-43a1-80be-c1d6cc570440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b7134-a150-4c82-86b5-8ed7ee4fed6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070c610d-af63-4f66-af3f-4f2c2ce7896d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6380b-22aa-4e86-8acc-34f73bb1dc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e0f3a-d44e-4547-bbbb-0e0935ea0297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4df10-24f7-44fd-91d8-850af070934a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebeac2e-624d-4454-904f-a0586292b2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef8047-eb2b-4324-afe8-78f4f7e290e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091544fe-d12f-487d-85a1-a878bfb363c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc02af-5f75-4479-95d8-2a11b0d80b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c94d9-ea99-4f27-9a3c-2cbd522335dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da7223-4621-4208-8713-dd2f22c09f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8ea43-527f-46ef-a984-1e36d34aadae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfaa95-d33e-4c06-8dad-ca470c14eab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a74604-3a1b-4c6e-97a7-75e5564bc855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fa507-1431-4d44-8888-b10285bc500b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec6ed8-a074-4eb5-84c0-2242ad3292f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce78df4-82ba-4e46-9e8e-7dc2915c2106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586e91f-36b2-4c8a-8027-adead27965a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aeb4ea-c6af-4162-8f6d-613a265a25fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05852ff-8e2e-41c3-91d7-24361a854e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66144c12-eddb-4e26-81ed-eba206b1b421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15700545-6afc-48d3-9bdf-27e0067b8441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a5eb7-fed0-4848-8e5a-0131d125663d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399d848-b38b-4657-8c31-dfd2c92044c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eca35f-71a6-4fce-8a3e-bf5fbe7434ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9d2e2-621b-4b6a-889b-932247b9ed39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94eee9-23e4-49a0-8177-78fde8211194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b09d4-c2fc-42c6-b197-bc24a7985c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8ed3c-4a32-45a3-8de2-5cf535c5eb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93265725-3b86-4982-8e7c-284e67e69abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12774656-5ae7-4430-a27d-4a680ec67343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49d263-71e4-49e5-8467-7c9d7cc9f13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7da60b-a922-4f8c-8695-550f357661b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21272005-3827-421a-9d25-384ecf66924f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
